{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHbeU9Dr7t7xMlbNQIRuRS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zb15/B2BNetworkWiki/blob/main/Functions_B2BNetworkWiki.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOXXiCcB32Ex"
      },
      "source": [
        "# Functions for Python package: B2BNetworkWiki"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Please cite the code and work in this repository as follows:\n",
        "\n",
        "Baruwa, Zsofia, & Li, Shujun & Zhu, Zhen. (2023). ... https://doi.org/ ...\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QzsLh5fnEjO4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KhB4NAsH975"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMYRKsCaG1Pc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87194261-3933-4802-97ca-642d8b7fb4ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install pyvis --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycountry --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjorEeQLXDki",
        "outputId": "a8501e98-ab44-46e0-9ebc-860a73cb1bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pycountry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycountry-convert --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRWAJ0jxYmM7",
        "outputId": "3f4572a0-cfcd-4cdb-af8e-b94984da536d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/229.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m225.3/229.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.0/229.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "zM3aShv3KN-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4jb9SSdqDB-"
      },
      "source": [
        "## FUNCTION 1. **choose_company(input_name, search_option='all')**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def searchCompanyByName(input_query, search_option):\n",
        "    # Importing the modules\n",
        "    import requests\n",
        "    import random\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    # Defining a list of user agents to alternate\n",
        "    user_agents = [\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Safari/605.1.15\",\n",
        "        \"Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0\",\n",
        "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Mobile/15E148 Safari/604.1\"\n",
        "    ]\n",
        "    query = ''\n",
        "    if search_option == 'all':\n",
        "        # Constructing the SPARQL query\n",
        "        query = f\"\"\" SELECT distinct ?item ?QID ?itemLabel ?itemDescription\n",
        "        WHERE{{\n",
        "          ?item rdfs:label ?itemLabel.\n",
        "          FILTER(REGEX(?itemLabel, ?name )).\n",
        "          VALUES ?name {{\"^{input_query}$\"@en \"^{input_query} \"@en \"^{input_query}, \"@en}} .\n",
        "          FILTER(LANG(?itemLabel)=\"en\").\n",
        "          ?item wdt:P31 ?type .\n",
        "          VALUES ?type {{wd:Q4830453 wd:Q783794 wd:Q6881511 wd:Q167037 wd:Q21980538 wd:Q891723 wd:Q786820 wd:Q43229 wd:Q1058914\n",
        "                                wd:Q18388277 wd:Q161726 wd:Q778575 wd:Q2005696 wd:Q108460239 wd:Q3477381 wd:Q270791 wd:Q936518\n",
        "                                wd:Q1934969 wd:Q2538889 wd:Q2995256 wd:Q1631129 wd:Q1276157 wd:Q5038204 wd:Q217107 wd:Q13235160\n",
        "                                wd:Q17377208 wd:Q740752 wd:Q249556}} . #to search multiple entities\n",
        "\n",
        "          ?article schema:about ?item .\n",
        "          ?article schema:inLanguage \"en\" .\n",
        "          ?article schema:isPartOf <https://en.wikipedia.org/>.\n",
        "        #the next 3 lines will give the QID in the column QID\n",
        "        #This query will bind the substring of ?uri starting from the 32nd character to a new variable ?code\n",
        "        BIND (STR (?item) AS ?uri) .\n",
        "        FILTER (STRSTARTS (?uri, \"http://www.wikidata.org/entity/Q\")) .\n",
        "        BIND (SUBSTR (?uri, 32) AS ?QID) .\n",
        "\n",
        "        SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "        }}\n",
        "        \"\"\"\n",
        "    elif search_option == 'exact':\n",
        "        query = f\"\"\" SELECT distinct ?item ?QID ?itemLabel ?itemDescription\n",
        "         WHERE{{\n",
        "           ?item rdfs:label ?itemLabel.\n",
        "           FILTER(REGEX(?itemLabel, ?name )).\n",
        "           VALUES ?name {{\"^{input_query}$\"@en}} .\n",
        "           FILTER(LANG(?itemLabel)=\"en\").\n",
        "           ?item wdt:P31 ?type .\n",
        "           VALUES ?type {{wd:Q4830453 wd:Q783794 wd:Q6881511 wd:Q167037 wd:Q21980538 wd:Q891723 wd:Q786820 wd:Q43229 wd:Q1058914\n",
        "                                 wd:Q18388277 wd:Q161726 wd:Q778575 wd:Q2005696 wd:Q108460239 wd:Q3477381 wd:Q270791 wd:Q936518\n",
        "                                 wd:Q1934969 wd:Q2538889 wd:Q2995256 wd:Q1631129 wd:Q1276157 wd:Q5038204 wd:Q217107 wd:Q13235160\n",
        "                                 wd:Q17377208 wd:Q740752 wd:Q249556}} . #to search multiple entities\n",
        "\n",
        "           ?article schema:about ?item .\n",
        "           ?article schema:inLanguage \"en\" .\n",
        "           ?article schema:isPartOf <https://en.wikipedia.org/>.\n",
        "         #the next 3 lines will give the QID in the column QID\n",
        "         #This query will bind the substring of ?uri starting from the 32nd character to a new variable ?code\n",
        "         BIND (STR (?item) AS ?uri) .\n",
        "         FILTER (STRSTARTS (?uri, \"http://www.wikidata.org/entity/Q\")) .\n",
        "         BIND (SUBSTR (?uri, 32) AS ?QID) .\n",
        "\n",
        "         SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "         }}\n",
        "         \"\"\"\n",
        "    elif search_option == 'space':\n",
        "      # Constructing the SPARQL query\n",
        "        query = f\"\"\" SELECT distinct ?item ?QID ?itemLabel ?itemDescription\n",
        "        WHERE{{\n",
        "          ?item rdfs:label ?itemLabel.\n",
        "          FILTER(REGEX(?itemLabel, ?name )).\n",
        "          VALUES ?name {{\"^{input_query} \"@en}} .\n",
        "          FILTER(LANG(?itemLabel)=\"en\").\n",
        "          ?item wdt:P31 ?type .\n",
        "          VALUES ?type {{wd:Q4830453 wd:Q783794 wd:Q6881511 wd:Q167037 wd:Q21980538 wd:Q891723 wd:Q786820 wd:Q43229 wd:Q1058914\n",
        "                                wd:Q18388277 wd:Q161726 wd:Q778575 wd:Q2005696 wd:Q108460239 wd:Q3477381 wd:Q270791 wd:Q936518\n",
        "                                wd:Q1934969 wd:Q2538889 wd:Q2995256 wd:Q1631129 wd:Q1276157 wd:Q5038204 wd:Q217107 wd:Q13235160\n",
        "                                wd:Q17377208 wd:Q740752 wd:Q249556}} . #to search multiple entities\n",
        "\n",
        "          ?article schema:about ?item .\n",
        "          ?article schema:inLanguage \"en\" .\n",
        "          ?article schema:isPartOf <https://en.wikipedia.org/>.\n",
        "        #the next 3 lines will give the QID in the column QID\n",
        "        #This query will bind the substring of ?uri starting from the 32nd character to a new variable ?code\n",
        "        BIND (STR (?item) AS ?uri) .\n",
        "        FILTER (STRSTARTS (?uri, \"http://www.wikidata.org/entity/Q\")) .\n",
        "        BIND (SUBSTR (?uri, 32) AS ?QID) .\n",
        "\n",
        "        SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "        }}\n",
        "        \"\"\"\n",
        "    elif search_option == 'space_comma':\n",
        "      # Constructing the SPARQL query\n",
        "        query = f\"\"\" SELECT distinct ?item ?QID ?itemLabel ?itemDescription\n",
        "        WHERE{{\n",
        "          ?item rdfs:label ?itemLabel.\n",
        "          FILTER(REGEX(?itemLabel, ?name )).\n",
        "          VALUES ?name {{\"^{input_query}, \"@en}} .\n",
        "          FILTER(LANG(?itemLabel)=\"en\").\n",
        "          ?item wdt:P31 ?type .\n",
        "          VALUES ?type {{wd:Q4830453 wd:Q783794 wd:Q6881511 wd:Q167037 wd:Q21980538 wd:Q891723 wd:Q786820 wd:Q43229 wd:Q1058914\n",
        "                                wd:Q18388277 wd:Q161726 wd:Q778575 wd:Q2005696 wd:Q108460239 wd:Q3477381 wd:Q270791 wd:Q936518\n",
        "                                wd:Q1934969 wd:Q2538889 wd:Q2995256 wd:Q1631129 wd:Q1276157 wd:Q5038204 wd:Q217107 wd:Q13235160\n",
        "                                wd:Q17377208 wd:Q740752 wd:Q249556}} . #to search multiple entities\n",
        "\n",
        "          ?article schema:about ?item .\n",
        "          ?article schema:inLanguage \"en\" .\n",
        "          ?article schema:isPartOf <https://en.wikipedia.org/>.\n",
        "        #the next 3 lines will give the QID in the column QID\n",
        "        #This query will bind the substring of ?uri starting from the 32nd character to a new variable ?code\n",
        "        BIND (STR (?item) AS ?uri) .\n",
        "        FILTER (STRSTARTS (?uri, \"http://www.wikidata.org/entity/Q\")) .\n",
        "        BIND (SUBSTR (?uri, 32) AS ?QID) .\n",
        "\n",
        "        SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "        }}\n",
        "        \"\"\"\n",
        "\n",
        "    # Choosing a random user agent from the list\n",
        "    user_agent = random.choice(user_agents)\n",
        "    # Sending the request to the Wikidata endpoint with the user agent header\n",
        "    response = requests.get(\"https://query.wikidata.org/sparql\", params={\"query\": query, \"format\": \"json\"}, headers={\"User-Agent\": user_agent})\n",
        "    # Parsing the response as JSON\n",
        "    data = response.json()\n",
        "    output = data[\"results\"][\"bindings\"]\n",
        "    return output"
      ],
      "metadata": {
        "id": "dNVDVGpOrGKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_VnB9uYqDB_"
      },
      "outputs": [],
      "source": [
        "# Importing the modules\n",
        "import requests\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Defining a function that takes an input from user and returns the Wikidata ID and label\n",
        "def choose_company(input_name, search_option='all'):\n",
        "    # remove any space at the beginning/end of the string\n",
        "    input_name = input_name.strip()\n",
        "    # Initializing an empty list to store the IDs and labels\n",
        "    ids_and_labels = []\n",
        "    # Make the query\n",
        "    results = searchCompanyByName(input_name, search_option)\n",
        "    if len(results) != 0:\n",
        "        # Looping through the results and appending them to the list\n",
        "        for result in results:\n",
        "            # Getting the Wikidata ID and label\n",
        "            wikidata_id = result[\"item\"][\"value\"].split(\"/\")[-1]\n",
        "            wikidata_label = result[\"itemLabel\"][\"value\"]\n",
        "            # Appending a tuple of ID and label to the list\n",
        "            ids_and_labels.append((wikidata_id, wikidata_label))\n",
        "    return ids_and_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--eaUGulMieG"
      },
      "source": [
        "## FUNCTION 2. get_companies_network(QIDs, num_runs=5,5,5,5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2xMkv1uZvnW"
      },
      "source": [
        "### Function to get parent info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82xXy8n_Yik2"
      },
      "outputs": [],
      "source": [
        "# Checking if entity has a parent, and if so how many\n",
        "def has_parent(items):\n",
        "    # Defining a list of user agents to alternate\n",
        "    user_agents = [\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Safari/605.1.15\",\n",
        "        \"Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0\",\n",
        "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Mobile/15E148 Safari/604.1\"\n",
        "    ]\n",
        "    # Constructing the SPARQL query\n",
        "    query = f\"\"\" SELECT ?parent (IF (BOUND(?parent), COUNT(DISTINCT ?parent), 0) AS ?count)\n",
        "          WHERE {{\n",
        "                VALUES ?item {{wd:{items}}}.\n",
        "                ?item wdt:P31 ?type .\n",
        "                VALUES ?type {{wd:Q4830453 wd:Q783794 wd:Q6881511 wd:Q167037 wd:Q21980538 wd:Q891723 wd:Q786820 wd:Q43229 wd:Q1058914\n",
        "                            wd:Q18388277 wd:Q161726 wd:Q778575 wd:Q2005696 wd:Q108460239 wd:Q3477381 wd:Q270791 wd:Q936518\n",
        "                            wd:Q1934969 wd:Q2538889 wd:Q2995256 wd:Q1631129 wd:Q1276157 wd:Q5038204 wd:Q217107 wd:Q13235160\n",
        "                            wd:Q17377208 wd:Q740752 wd:Q249556}} . #to search multiple entities\n",
        "                ?article schema:about ?item .\n",
        "                ?article schema:inLanguage \"en\" .\n",
        "                ?article schema:isPartOf <https://en.wikipedia.org/>.\n",
        "\n",
        "                OPTIONAL {{ ?item wdt:P749 ?parent. }}\n",
        "\n",
        "                SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "                }}\n",
        "                GROUP BY ?parent\n",
        "    \"\"\"\n",
        "    # Choosing a random user agent from the list\n",
        "    user_agent = random.choice(user_agents)\n",
        "    # Sending the request to the Wikidata endpoint with the user agent header\n",
        "    response = requests.get(\"https://query.wikidata.org/sparql\", params={\"query\": query, \"format\": \"json\"}, headers={\"User-Agent\": user_agent})\n",
        "    # Parsing the response as JSON\n",
        "    data = response.json()\n",
        "    # Extracting all the results\n",
        "    results = data[\"results\"][\"bindings\"]\n",
        "    # Check if results is empty\n",
        "    if not results:\n",
        "      # Return zero\n",
        "      return 0\n",
        "    else:\n",
        "      # Get the count value\n",
        "      counts = results[0][\"count\"][\"value\"]\n",
        "      # Make counts integers\n",
        "      counts = int(counts)\n",
        "      # Return the count as an integer\n",
        "      return counts\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr2tkJbFZG1_"
      },
      "outputs": [],
      "source": [
        "# Defining a function that takes a Wikidata ID and returns a dictionary of information about its industry, parent organizations and subsidiaries\n",
        "def get_company_parent_info(wikidata_id):\n",
        "    # Defining a list of user agents to alternate\n",
        "    user_agents = [\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Safari/605.1.15\",\n",
        "        \"Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0\",\n",
        "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Mobile/15E148 Safari/604.1\"\n",
        "    ]\n",
        "    # Constructing the SPARQL query\n",
        "    query = f\"\"\"\n",
        "    SELECT DISTINCT ?item\n",
        "                    (REPLACE(STR(?item), \"http://www.wikidata.org/entity/\", \"\") AS ?QID)\n",
        "                    ?itemLabel\n",
        "                    ?itemcountryLabel\n",
        "                    (GROUP_CONCAT ( DISTINCT ?industryLabel; separator=\"; \") AS ?industries)\n",
        "                    ?parent\n",
        "                    (REPLACE(STR(?parent), \"http://www.wikidata.org/entity/\", \"\") AS ?pQID)\n",
        "                    ?parentLabel\n",
        "                    ?parentcountryLabel\n",
        "                    (GROUP_CONCAT ( DISTINCT ?pindustryLabel; separator=\"; \") AS ?pindustries)\n",
        "                    ?proportion\n",
        "                    ?proportionofLabel\n",
        "                    ?pointoftime\n",
        "                    ?starttime\n",
        "                    ?endtime\n",
        "    WHERE {{\n",
        "      VALUES ?item {{wd:{wikidata_id}}}.\n",
        "      ?item wdt:P31 ?type .\n",
        "      ?article schema:about ?item .\n",
        "      ?article schema:inLanguage \"en\" .\n",
        "      ?article schema:isPartOf <https://en.wikipedia.org/>.\n",
        "\n",
        "        OPTIONAL {{ ?item rdfs:label ?itemLabel. FILTER(LANG(?itemLabel) = \"en\") }}\n",
        "        OPTIONAL {{ ?item     wdt:P17    ?itemcountry .\n",
        "                    ?itemcountry  rdfs:label  ?itemcountryLabel\n",
        "                    FILTER ( LANGMATCHES ( LANG ( ?itemcountryLabel ), \"en\" ) )\n",
        "                  }}\n",
        "        OPTIONAL {{ ?item     wdt:P452    ?industry .\n",
        "                    ?industry  rdfs:label  ?industryLabel\n",
        "                    FILTER ( LANGMATCHES ( LANG ( ?industryLabel ), \"en\" ) )\n",
        "                  }}\n",
        "        OPTIONAL {{ ?item wdt:P749 ?parent. }}\n",
        "        OPTIONAL {{ ?parent     wdt:P452    ?pindustry .\n",
        "                    ?pindustry  rdfs:label  ?pindustryLabel\n",
        "                    FILTER ( LANGMATCHES ( LANG ( ?pindustryLabel ), \"en\" ) )\n",
        "                  }}\n",
        "        OPTIONAL {{ ?parent     wdt:P17    ?parentcountry .\n",
        "                    ?parentcountry  rdfs:label  ?parentcountryLabel\n",
        "                    FILTER ( LANGMATCHES ( LANG ( ?parentcountryLabel ), \"en\" ) )\n",
        "                  }}\n",
        "        OPTIONAL {{ ?item  p:P749 [ps:P749 ?parent; pq:P1107 ?proportion] }}\n",
        "        OPTIONAL {{\n",
        "                  ?item  p:P749 [ps:P749 ?parent; pq:P642 ?proportionof] .\n",
        "                  FILTER(?proportionof = wd:Q144368)\n",
        "                 }}\n",
        "        OPTIONAL {{ ?item p:P749 [ps:P749 ?parent; pq:P585 ?pointoftime] }}\n",
        "        OPTIONAL {{\n",
        "                  ?item p:P749 [ps:P749 ?parent; pq:P585 ?pointoftime] .\n",
        "                  FILTER(BOUND(?pointoftime) && DATATYPE(?pointoftime) = xsd:dateTime).\n",
        "                  # get the latest record first\n",
        "                  BIND(NOW() - ?pointoftime AS ?distance).\n",
        "                  FILTER (MIN (?distance)) .\n",
        "                 }}\n",
        "        OPTIONAL {{ ?item  p:P749 [ps:P749 ?parent; pq:P580 ?starttime] }}\n",
        "        OPTIONAL {{ ?item  p:P749 [ps:P749 ?parent; pq:P582 ?endtime] }}\n",
        "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "    }}\n",
        "    GROUP BY  ?item ?QID ?itemLabel ?itemcountryLabel ?parent ?pQID ?parentLabel ?parentcountryLabel ?proportion ?proportionofLabel ?pointoftime ?starttime ?endtime\n",
        "    ORDER BY ?parentLabel\n",
        "    LIMIT 10000\n",
        "    \"\"\"\n",
        "\n",
        "    # Choosing a random user agent from the list\n",
        "    user_agent = random.choice(user_agents)\n",
        "    # defining url\n",
        "    url = 'https://query.wikidata.org/sparql'\n",
        "    # Sending the request to the Wikidata endpoint with the user agent header\n",
        "    r = requests.get(url, params={'format': 'json', 'query': query}, headers={\"User-Agent\": user_agent})\n",
        "    # extracts the results and bindings keys from the JSON response returned by the Wikidata SPARQL endpoint\n",
        "    data = r.json()['results']['bindings']\n",
        "    # converts the JSON data into a pandas DataFrame\n",
        "    df = pd.json_normalize(data)\n",
        "    #  checks if an element is a dictionary and if so, extracts the value key from it\n",
        "    df = df.applymap(lambda x: x['value'] if isinstance(x, dict) else x)\n",
        "    # selects only the columns that end with .value.\n",
        "    df = df[[col for col in df.columns if col.endswith('.value')]]\n",
        "    # removes the .value suffix from each column name\n",
        "    df.columns = [col.split('.')[0] for col in df.columns]\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlYiNtxEZ3EH"
      },
      "source": [
        "### Function to get owned by info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCYJ4yvLZsSo"
      },
      "outputs": [],
      "source": [
        "# Checking if entity has a ownedby (which is also a type of company), and if so how many\n",
        "def has_ownedby(items):\n",
        "    # Defining a list of user agents to alternate\n",
        "    user_agents = [\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Safari/605.1.15\",\n",
        "        \"Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0\",\n",
        "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Mobile/15E148 Safari/604.1\"\n",
        "    ]\n",
        "    # Constructing the SPARQL query\n",
        "    query = f\"\"\" SELECT ?ownedby (IF (BOUND(?ownedby), COUNT(DISTINCT ?ownedby), 0) AS ?count)\n",
        "          WHERE {{\n",
        "                VALUES ?item {{wd:{items}}}.\n",
        "                ?item wdt:P31 ?type .\n",
        "                VALUES ?type {{wd:Q4830453 wd:Q783794 wd:Q6881511 wd:Q167037 wd:Q21980538 wd:Q891723 wd:Q786820 wd:Q43229 wd:Q1058914\n",
        "                            wd:Q18388277 wd:Q161726 wd:Q778575 wd:Q2005696 wd:Q108460239 wd:Q3477381 wd:Q270791 wd:Q936518\n",
        "                            wd:Q1934969 wd:Q2538889 wd:Q2995256 wd:Q1631129 wd:Q1276157 wd:Q5038204 wd:Q217107 wd:Q13235160\n",
        "                            wd:Q17377208 wd:Q740752 wd:Q249556}} . #to search multiple entities\n",
        "                ?article schema:about ?item .\n",
        "                ?article schema:inLanguage \"en\" .\n",
        "                ?article schema:isPartOf <https://en.wikipedia.org/>.\n",
        "\n",
        "                OPTIONAL {{ ?item wdt:P127 ?ownedby. }}\n",
        "\n",
        "                SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "                }}\n",
        "                GROUP BY ?ownedby\n",
        "    \"\"\"\n",
        "    # Choosing a random user agent from the list\n",
        "    user_agent = random.choice(user_agents)\n",
        "    # Sending the request to the Wikidata endpoint with the user agent header\n",
        "    response = requests.get(\"https://query.wikidata.org/sparql\", params={\"query\": query, \"format\": \"json\"}, headers={\"User-Agent\": user_agent})\n",
        "    # Parsing the response as JSON\n",
        "    data = response.json()\n",
        "    # Extracting all the results\n",
        "    results = data[\"results\"][\"bindings\"]\n",
        "    # Check if results is empty\n",
        "    if not results:\n",
        "      # Return zero\n",
        "      return 0\n",
        "    else:\n",
        "      # Get the count value\n",
        "      counts = results[0][\"count\"][\"value\"]\n",
        "      # Make counts integers\n",
        "      counts = int(counts)\n",
        "      # Return the count as an integer\n",
        "      return counts\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpQ92LK7aGg7"
      },
      "outputs": [],
      "source": [
        "# Defining a function that takes a Wikidata ID and returns a dictionary of information about its industry, parent organizations and subsidiaries\n",
        "def get_company_ownedby_info(wikidata_id):\n",
        "    # Defining a list of user agents to alternate\n",
        "    user_agents = [\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Safari/605.1.15\",\n",
        "        \"Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0\",\n",
        "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Mobile/15E148 Safari/604.1\"\n",
        "    ]\n",
        "    # Constructing the SPARQL query\n",
        "    query = f\"\"\"\n",
        "    SELECT DISTINCT ?item\n",
        "                    (REPLACE(STR(?item), \"http://www.wikidata.org/entity/\", \"\") AS ?QID)\n",
        "                    ?itemLabel\n",
        "                    ?itemcountryLabel\n",
        "                    (GROUP_CONCAT ( DISTINCT ?industryLabel; separator=\"; \") AS ?industries)\n",
        "                    ?ownedby\n",
        "                    (REPLACE(STR(?ownedby), \"http://www.wikidata.org/entity/\", \"\") AS ?obQID)\n",
        "                    ?ownedbyLabel\n",
        "                    ?ownedbycountryLabel\n",
        "                    (GROUP_CONCAT ( DISTINCT ?obindustryLabel; separator=\"; \") AS ?obindustries)\n",
        "                    ?proportion\n",
        "                    ?proportionofLabel\n",
        "                    ?pointoftime\n",
        "                    ?starttime\n",
        "                    ?endtime\n",
        "    WHERE {{\n",
        "      VALUES ?item {{wd:{wikidata_id}}}.\n",
        "      ?item wdt:P31 ?type .\n",
        "      ?article schema:about ?item .\n",
        "      ?article schema:inLanguage \"en\" .\n",
        "      ?article schema:isPartOf <https://en.wikipedia.org/>.\n",
        "\n",
        "        OPTIONAL {{ ?item rdfs:label ?itemLabel. FILTER(LANG(?itemLabel) = \"en\") }}\n",
        "        OPTIONAL {{ ?item     wdt:P17    ?itemcountry .\n",
        "                    ?itemcountry  rdfs:label  ?itemcountryLabel\n",
        "                    FILTER ( LANGMATCHES ( LANG ( ?itemcountryLabel ), \"en\" ) )\n",
        "                  }}\n",
        "        OPTIONAL {{ ?item     wdt:P452    ?industry .\n",
        "                    ?industry  rdfs:label  ?industryLabel\n",
        "                    FILTER ( LANGMATCHES ( LANG ( ?industryLabel ), \"en\" ) )\n",
        "                  }}\n",
        "        OPTIONAL {{ ?item wdt:P127 ?ownedby. }}\n",
        "        OPTIONAL {{ ?ownedby     wdt:P452    ?obindustry .\n",
        "                    ?obindustry  rdfs:label  ?obindustryLabel\n",
        "                    FILTER ( LANGMATCHES ( LANG ( ?obindustryLabel ), \"en\" ) )\n",
        "                  }}\n",
        "        OPTIONAL {{ ?ownedby     wdt:P17    ?ownedbycountry .\n",
        "                    ?ownedbycountry  rdfs:label  ?ownedbycountryLabel\n",
        "                    FILTER ( LANGMATCHES ( LANG ( ?ownedbycountryLabel ), \"en\" ) )\n",
        "                  }}\n",
        "        OPTIONAL {{ ?item  p:P127 [ps:P127 ?ownedby; pq:P1107 ?proportion] }}\n",
        "        OPTIONAL {{\n",
        "                  ?item  p:P127 [ps:P127 ?ownedby; pq:P642 ?proportionof] .\n",
        "                  FILTER(?proportionof = wd:Q144368)\n",
        "                 }}\n",
        "        OPTIONAL {{ ?item p:P127 [ps:P127 ?ownedby; pq:P585 ?pointoftime] }}\n",
        "        OPTIONAL {{\n",
        "                  ?item p:P127 [ps:P127 ?ownedby; pq:P585 ?pointoftime] .\n",
        "                  FILTER(BOUND(?pointoftime) && DATATYPE(?pointoftime) = xsd:dateTime).\n",
        "                  # get the latest record first\n",
        "                  BIND(NOW() - ?pointoftime AS ?distance).\n",
        "                  FILTER (MIN (?distance)) .\n",
        "                 }}\n",
        "        OPTIONAL {{ ?item  p:P127 [ps:P127 ?ownedby; pq:P580 ?starttime] }}\n",
        "        OPTIONAL {{ ?item  p:P127 [ps:P127 ?ownedby; pq:P582 ?endtime] }}\n",
        "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "    }}\n",
        "    GROUP BY  ?item ?QID ?itemLabel ?itemcountryLabel ?ownedby ?obQID ?ownedbyLabel ?ownedbycountryLabel ?proportion ?proportionofLabel ?pointoftime ?starttime ?endtime\n",
        "    ORDER BY ?ownedbyLabel\n",
        "    LIMIT 10000\n",
        "    \"\"\"\n",
        "\n",
        "     # Choosing a random user agent from the list\n",
        "    user_agent = random.choice(user_agents)\n",
        "    # defining url\n",
        "    url = 'https://query.wikidata.org/sparql'\n",
        "    # Sending the request to the Wikidata endpoint with the user agent header\n",
        "    r = requests.get(url, params={'format': 'json', 'query': query}, headers={\"User-Agent\": user_agent})\n",
        "    # extracts the results and bindings keys from the JSON response returned by the Wikidata SPARQL endpoint\n",
        "    data = r.json()['results']['bindings']\n",
        "    # converts the JSON data into a pandas DataFrame\n",
        "    df = pd.json_normalize(data)\n",
        "    #  checks if an element is a dictionary and if so, extracts the value key from it\n",
        "    df = df.applymap(lambda x: x['value'] if isinstance(x, dict) else x)\n",
        "    # selects only the columns that end with .value.\n",
        "    df = df[[col for col in df.columns if col.endswith('.value')]]\n",
        "    # removes the .value suffix from each column name\n",
        "    df.columns = [col.split('.')[0] for col in df.columns]\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBkm1hjpZ7-3"
      },
      "source": [
        "### Function to get subsidiary info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4e9uWedaMmg"
      },
      "outputs": [],
      "source": [
        "# Checking if entity has a subsidiary (which is also a type of company), and if so how many\n",
        "def has_subsidiary(items):\n",
        "    # Defining a list of user agents to alternate\n",
        "    user_agents = [\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Safari/605.1.15\",\n",
        "        \"Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0\",\n",
        "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Mobile/15E148 Safari/604.1\"\n",
        "    ]\n",
        "    # Constructing the SPARQL query\n",
        "    query = f\"\"\" SELECT ?subsidiary (IF (BOUND(?subsidiary), COUNT(DISTINCT ?subsidiary), 0) AS ?count)\n",
        "          WHERE {{\n",
        "                VALUES ?item {{wd:{items}}}.\n",
        "                ?item wdt:P31 ?type .\n",
        "                VALUES ?type {{wd:Q4830453 wd:Q783794 wd:Q6881511 wd:Q167037 wd:Q21980538 wd:Q891723 wd:Q786820 wd:Q43229 wd:Q1058914\n",
        "                            wd:Q18388277 wd:Q161726 wd:Q778575 wd:Q2005696 wd:Q108460239 wd:Q3477381 wd:Q270791 wd:Q936518\n",
        "                            wd:Q1934969 wd:Q2538889 wd:Q2995256 wd:Q1631129 wd:Q1276157 wd:Q5038204 wd:Q217107 wd:Q13235160\n",
        "                            wd:Q17377208 wd:Q740752 wd:Q249556}} . #to search multiple entities\n",
        "                ?article schema:about ?item .\n",
        "                ?article schema:inLanguage \"en\" .\n",
        "                ?article schema:isPartOf <https://en.wikipedia.org/>.\n",
        "\n",
        "                OPTIONAL {{ ?item wdt:P355 ?subsidiary. }}\n",
        "\n",
        "                SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "                }}\n",
        "                GROUP BY ?subsidiary\n",
        "    \"\"\"\n",
        "    # Choosing a random user agent from the list\n",
        "    user_agent = random.choice(user_agents)\n",
        "    # Sending the request to the Wikidata endpoint with the user agent header\n",
        "    response = requests.get(\"https://query.wikidata.org/sparql\", params={\"query\": query, \"format\": \"json\"}, headers={\"User-Agent\": user_agent})\n",
        "    # Parsing the response as JSON\n",
        "    data = response.json()\n",
        "    # Extracting all the results\n",
        "    results = data[\"results\"][\"bindings\"]\n",
        "    # Check if results is empty\n",
        "    if not results:\n",
        "      # Return zero\n",
        "      return 0\n",
        "    else:\n",
        "      # Get the count value\n",
        "      counts = results[0][\"count\"][\"value\"]\n",
        "      # Make counts integers\n",
        "      counts = int(counts)\n",
        "      # Return the count as an integer\n",
        "      return counts\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BPTFdQmaSDL"
      },
      "outputs": [],
      "source": [
        "# getting company's subsidaries\n",
        "def get_company_subsidiary_info(wikidata_id):\n",
        "    import requests\n",
        "    import pandas as pd\n",
        "    # Defining a list of user agents to alternate\n",
        "    user_agents = [\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Safari/605.1.15\",\n",
        "        \"Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0\",\n",
        "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Mobile/15E148 Safari/604.1\"\n",
        "    ]\n",
        "    query = f\"\"\"\n",
        "        SELECT DISTINCT ?item\n",
        "                        (REPLACE(STR(?item), \"http://www.wikidata.org/entity/\", \"\") AS ?QID)\n",
        "                        ?itemLabel\n",
        "                        ?itemcountryLabel\n",
        "                        (GROUP_CONCAT ( DISTINCT ?industryLabel; separator=\"; \") AS ?industries)\n",
        "                        ?subsidiary\n",
        "                        (REPLACE(STR(?subsidiary), \"http://www.wikidata.org/entity/\", \"\") AS ?sQID)\n",
        "                        ?subsidiaryLabel\n",
        "                        ?subsidiarycountryLabel\n",
        "                        (GROUP_CONCAT ( DISTINCT ?sindustryLabel; separator=\"; \") AS ?sindustries)\n",
        "                        ?proportion\n",
        "                        ?proportionofLabel\n",
        "                        ?pointoftime\n",
        "                        ?starttime\n",
        "                        ?endtime\n",
        "        WHERE {{\n",
        "          VALUES ?item {{wd:{wikidata_id}}}.\n",
        "          ?item wdt:P31 ?type .\n",
        "          ?article schema:about ?item .\n",
        "          ?article schema:inLanguage \"en\" .\n",
        "          ?article schema:isPartOf <https://en.wikipedia.org/>.\n",
        "\n",
        "            OPTIONAL {{ ?item rdfs:label ?itemLabel. FILTER(LANG(?itemLabel) = \"en\") }}\n",
        "            OPTIONAL {{ ?item     wdt:P17    ?itemcountry .\n",
        "                    ?itemcountry  rdfs:label  ?itemcountryLabel\n",
        "                    FILTER ( LANGMATCHES ( LANG ( ?itemcountryLabel ), \"en\" ) )\n",
        "                  }}\n",
        "            OPTIONAL {{ ?item     wdt:P452    ?industry .\n",
        "                        ?industry  rdfs:label  ?industryLabel\n",
        "                        FILTER ( LANGMATCHES ( LANG ( ?industryLabel ), \"en\" ) )\n",
        "                      }}\n",
        "            OPTIONAL {{ ?item wdt:P355 ?subsidiary. }}\n",
        "            OPTIONAL {{ ?subsidiary     wdt:P452    ?sindustry .\n",
        "                        ?sindustry  rdfs:label  ?sindustryLabel\n",
        "                        FILTER ( LANGMATCHES ( LANG ( ?sindustryLabel ), \"en\" ) )\n",
        "                      }}\n",
        "            OPTIONAL {{ ?subsidiary     wdt:P17    ?subsidiarycountry .\n",
        "                    ?subsidiarycountry  rdfs:label  ?subsidiarycountryLabel\n",
        "                    FILTER ( LANGMATCHES ( LANG ( ?subsidiarycountryLabel ), \"en\" ) )\n",
        "                  }}\n",
        "            OPTIONAL {{ ?item  p:P355 [ps:P355 ?subsidiary; pq:P1107 ?proportion] }}\n",
        "            OPTIONAL {{\n",
        "                        ?item  p:P355 [ps:P355 ?subsidiary; pq:P642 ?proportionof] .\n",
        "                        FILTER(?proportionof = wd:Q144368)\n",
        "                     }}\n",
        "            OPTIONAL {{ ?item p:P355 [ps:P355 ?subsidiary; pq:P585 ?pointoftime] }}\n",
        "            OPTIONAL {{\n",
        "                        ?item p:P355 [ps:P355 ?subsidiary; pq:P585 ?pointoftime] .\n",
        "                        FILTER(BOUND(?pointoftime) && DATATYPE(?pointoftime) = xsd:dateTime).\n",
        "                        # get the latest record first\n",
        "                        BIND(NOW() - ?pointoftime AS ?distance).\n",
        "                        FILTER (MIN (?distance)) .\n",
        "                      }}\n",
        "            OPTIONAL {{ ?item  p:P355 [ps:P355 ?subsidiary; pq:P580 ?starttime] }}\n",
        "            OPTIONAL {{ ?item  p:P355 [ps:P355 ?subsidiary; pq:P582 ?endtime] }}\n",
        "          SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "        }}\n",
        "        GROUP BY  ?item ?QID ?itemLabel ?itemcountryLabel ?subsidiary ?sQID ?subsidiaryLabel ?subsidiarycountryLabel ?proportion ?proportionofLabel ?pointoftime ?starttime ?endtime\n",
        "        ORDER BY ?subsidiaryLabel\n",
        "        LIMIT 10000\n",
        "        \"\"\"\n",
        "\n",
        "    # Choosing a random user agent from the list\n",
        "    user_agent = random.choice(user_agents)\n",
        "    # defining url\n",
        "    url = 'https://query.wikidata.org/sparql'\n",
        "    # Sending the request to the Wikidata endpoint with the user agent header\n",
        "    r = requests.get(url, params={'format': 'json', 'query': query}, headers={\"User-Agent\": user_agent})\n",
        "    # extracts the results and bindings keys from the JSON response returned by the Wikidata SPARQL endpoint\n",
        "    data = r.json()['results']['bindings']\n",
        "    # converts the JSON data into a pandas DataFrame\n",
        "    df = pd.json_normalize(data)\n",
        "    #  checks if an element is a dictionary and if so, extracts the value key from it\n",
        "    df = df.applymap(lambda x: x['value'] if isinstance(x, dict) else x)\n",
        "    # selects only the columns that end with .value.\n",
        "    df = df[[col for col in df.columns if col.endswith('.value')]]\n",
        "    # removes the .value suffix from each column name\n",
        "    df.columns = [col.split('.')[0] for col in df.columns]\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-XT2LbzaTlN"
      },
      "source": [
        "### Function to get owner of info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsqZeI5Na3G0"
      },
      "outputs": [],
      "source": [
        "# Checking if entity has a ownerof (which is also a type of company), and if so how many\n",
        "def has_ownerof(items):\n",
        "    # Defining a list of user agents to alternate\n",
        "    user_agents = [\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Safari/605.1.15\",\n",
        "        \"Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0\",\n",
        "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Mobile/15E148 Safari/604.1\"\n",
        "    ]\n",
        "    # Constructing the SPARQL query\n",
        "    query = f\"\"\" SELECT ?ownerof (IF (BOUND(?ownerof), COUNT(DISTINCT ?ownerof), 0) AS ?count)\n",
        "          WHERE {{\n",
        "                VALUES ?item {{wd:{items}}}.\n",
        "                ?item wdt:P31 ?type .\n",
        "                VALUES ?type {{wd:Q4830453 wd:Q783794 wd:Q6881511 wd:Q167037 wd:Q21980538 wd:Q891723 wd:Q786820 wd:Q43229 wd:Q1058914\n",
        "                            wd:Q18388277 wd:Q161726 wd:Q778575 wd:Q2005696 wd:Q108460239 wd:Q3477381 wd:Q270791 wd:Q936518\n",
        "                            wd:Q1934969 wd:Q2538889 wd:Q2995256 wd:Q1631129 wd:Q1276157 wd:Q5038204 wd:Q217107 wd:Q13235160\n",
        "                            wd:Q17377208 wd:Q740752 wd:Q249556}} . #to search multiple entities\n",
        "                ?article schema:about ?item .\n",
        "                ?article schema:inLanguage \"en\" .\n",
        "                ?article schema:isPartOf <https://en.wikipedia.org/>.\n",
        "\n",
        "                OPTIONAL {{ ?item wdt:P1830 ?ownerof. }}\n",
        "\n",
        "                SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "                }}\n",
        "                GROUP BY ?ownerof\n",
        "    \"\"\"\n",
        "    # Choosing a random user agent from the list\n",
        "    user_agent = random.choice(user_agents)\n",
        "    # Sending the request to the Wikidata endpoint with the user agent header\n",
        "    response = requests.get(\"https://query.wikidata.org/sparql\", params={\"query\": query, \"format\": \"json\"}, headers={\"User-Agent\": user_agent})\n",
        "    # Parsing the response as JSON\n",
        "    data = response.json()\n",
        "    # Extracting all the results\n",
        "    results = data[\"results\"][\"bindings\"]\n",
        "    # Check if results is empty\n",
        "    if not results:\n",
        "      # Return zero\n",
        "      return 0\n",
        "    else:\n",
        "      # Get the count value\n",
        "      counts = results[0][\"count\"][\"value\"]\n",
        "      # Make counts integers\n",
        "      counts = int(counts)\n",
        "      # Return the count as an integer\n",
        "      return counts\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTiR6G3McOZv"
      },
      "outputs": [],
      "source": [
        "# Defining a function that takes a Wikidata ID and returns a dictionary of information about its industry, parent organizations and subsidiaries\n",
        "def get_company_ownerof_info(wikidata_id):\n",
        "    # Defining a list of user agents to alternate\n",
        "    user_agents = [\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Safari/605.1.15\",\n",
        "        \"Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0\",\n",
        "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Mobile/15E148 Safari/604.1\"\n",
        "    ]\n",
        "    # Constructing the SPARQL query\n",
        "    query = f\"\"\"\n",
        "    SELECT DISTINCT ?item\n",
        "                    (REPLACE(STR(?item), \"http://www.wikidata.org/entity/\", \"\") AS ?QID)\n",
        "                    ?itemLabel\n",
        "                    ?itemcountryLabel\n",
        "                    (GROUP_CONCAT ( DISTINCT ?industryLabel; separator=\"; \") AS ?industries)\n",
        "                    ?ownerof\n",
        "                    (REPLACE(STR(?ownerof), \"http://www.wikidata.org/entity/\", \"\") AS ?ooQID)\n",
        "                    ?ownerofLabel\n",
        "                    ?ownerofcountryLabel\n",
        "                    (GROUP_CONCAT ( DISTINCT ?ooindustryLabel; separator=\"; \") AS ?ooindustries)\n",
        "                    ?proportion\n",
        "                    ?proportionofLabel\n",
        "                    ?pointoftime\n",
        "                    ?starttime\n",
        "                    ?endtime\n",
        "    WHERE {{\n",
        "      VALUES ?item {{wd:{wikidata_id}}}.\n",
        "      ?item wdt:P31 ?type .\n",
        "      ?article schema:about ?item .\n",
        "      ?article schema:inLanguage \"en\" .\n",
        "      ?article schema:isPartOf <https://en.wikipedia.org/>.\n",
        "\n",
        "        OPTIONAL {{ ?item rdfs:label ?companyLabel. FILTER(LANG(?companyLabel) = \"en\") }}\n",
        "        OPTIONAL {{ ?item     wdt:P17    ?itemcountry .\n",
        "                    ?itemcountry  rdfs:label  ?itemcountryLabel\n",
        "                    FILTER ( LANGMATCHES ( LANG ( ?itemcountryLabel ), \"en\" ) )\n",
        "                  }}\n",
        "        OPTIONAL {{ ?item     wdt:P452    ?industry .\n",
        "                    ?industry  rdfs:label  ?industryLabel\n",
        "                    FILTER ( LANGMATCHES ( LANG ( ?industryLabel ), \"en\" ) )\n",
        "                  }}\n",
        "        OPTIONAL {{ ?item wdt:P1830 ?ownerof. }}\n",
        "        OPTIONAL {{ ?ownerof     wdt:P452    ?ooindustry .\n",
        "                    ?ooindustry  rdfs:label  ?ooindustryLabel\n",
        "                    FILTER ( LANGMATCHES ( LANG ( ?ooindustryLabel ), \"en\" ) )\n",
        "                  }}\n",
        "        OPTIONAL {{ ?ownerof     wdt:P17    ?ownerofcountry .\n",
        "                    ?ownerofcountry  rdfs:label  ?ownerofcountryLabel\n",
        "                    FILTER ( LANGMATCHES ( LANG ( ?ownerofcountryLabel ), \"en\" ) )\n",
        "                  }}\n",
        "        OPTIONAL {{ ?item  p:P1830 [ps:P1830 ?ownerof; pq:P1107 ?proportion] }}\n",
        "        OPTIONAL {{\n",
        "                    ?item  p:P1830 [ps:P1830 ?ownerof; pq:P642 ?proportionof] .\n",
        "                    FILTER(?proportionof = wd:Q144368)\n",
        "                  }}\n",
        "        OPTIONAL {{ ?item p:P1830 [ps:P1830 ?ownerof; pq:P585 ?pointoftime] }}\n",
        "        OPTIONAL {{\n",
        "                    ?item p:P1830 [ps:P1830 ?ownerof; pq:P585 ?pointoftime] .\n",
        "                    FILTER(BOUND(?pointoftime) && DATATYPE(?pointoftime) = xsd:dateTime).\n",
        "                    # get the latest record first\n",
        "                    BIND(NOW() - ?pointoftime AS ?distance).\n",
        "                    FILTER (MIN (?distance)) .\n",
        "                  }}\n",
        "        OPTIONAL {{ ?item  p:P1830 [ps:P1830 ?ownerof; pq:P580 ?starttime] }}\n",
        "        OPTIONAL {{ ?item  p:P1830 [ps:P1830 ?ownerof; pq:P582 ?endtime] }}\n",
        "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "    }}\n",
        "    GROUP BY  ?item ?QID ?itemLabel ?itemcountryLabel ?ownerof ?ooQID ?ownerofLabel ?ownerofcountryLabel ?proportion ?proportionofLabel ?pointoftime ?starttime ?endtime\n",
        "    ORDER BY ?ownerofLabel\n",
        "    LIMIT 10000\n",
        "    \"\"\"\n",
        "\n",
        "    # Choosing a random user agent from the list\n",
        "    user_agent = random.choice(user_agents)\n",
        "    # defining url\n",
        "    url = 'https://query.wikidata.org/sparql'\n",
        "    # Sending the request to the Wikidata endpoint with the user agent header\n",
        "    r = requests.get(url, params={'format': 'json', 'query': query}, headers={\"User-Agent\": user_agent})\n",
        "    # extracts the results and bindings keys from the JSON response returned by the Wikidata SPARQL endpoint\n",
        "    data = r.json()['results']['bindings']\n",
        "    # converts the JSON data into a pandas DataFrame\n",
        "    df = pd.json_normalize(data)\n",
        "    #  checks if an element is a dictionary and if so, extracts the value key from it\n",
        "    df = df.applymap(lambda x: x['value'] if isinstance(x, dict) else x)\n",
        "    # selects only the columns that end with .value.\n",
        "    df = df[[col for col in df.columns if col.endswith('.value')]]\n",
        "    # removes the .value suffix from each column name\n",
        "    df.columns = [col.split('.')[0] for col in df.columns]\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FQPEL9ie3eV"
      },
      "source": [
        "### Define the main function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tidy_each_df(df):\n",
        "  #split companies' industries string and return them as list, then drop duplicates, and finally return as string again\n",
        "  df.iloc[:, 4] = df.iloc[:,4].str.split(\"; \").apply(lambda x: \"; \".join(set(word.lower() for word in x)))\n",
        "  # drop parents' industries duplicates, then return as string again\n",
        "  df.iloc[:, 9] = df.iloc[:, 9].str.split(\"; \").apply(lambda x: \"; \".join(set(word.lower() for word in x)))\n",
        "  # check if all proprtions are between 0 and 1, if above 1, then devide by 100, otherwise leave it\n",
        "  if 'proportion' in df.columns:\n",
        "    df['proportion'] = df['proportion'].astype(float)\n",
        "    df['proportion'] = np.where(df['proportion'].isnull(), df['proportion'],\n",
        "                                np.where(df['proportion'] > 1, df['proportion']/100,\n",
        "                                         df['proportion']))\n",
        "  df = df.drop_duplicates().reset_index(drop=True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "8IKYifRUHlDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUX8xKN_9_L4"
      },
      "outputs": [],
      "source": [
        "# companies_network function\n",
        "\n",
        "# define the main function\n",
        "def get_companies_network(ids, levels=(5,5,5,5)):\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "  # Create an empty dataframe for parent\n",
        "  p_df = pd.DataFrame(columns=['item', 'QID', 'itemLabel', 'itemcountryLabel', 'industries', 'parent', \\\n",
        "                               'pQID', 'parentLabel', 'parentcountryLabel', 'pindustries', 'proportion', \\\n",
        "                               'proportionofLabel', 'pointoftime', 'starttime', 'endtime'])\n",
        "  # Create an empty dataframe for owned by\n",
        "  ob_df = pd.DataFrame(columns=['item', 'QID', 'itemLabel', 'itemcountryLabel', 'industries', 'ownedby', \\\n",
        "                                'obQID', 'ownedbyLabel', 'ownedbycountryLabel', 'obindustries', 'proportion', \\\n",
        "                                'proportionofLabel', 'pointoftime', 'starttime', 'endtime'])\n",
        "  # Create an empty dataframe for subsidiary\n",
        "  s_df = pd.DataFrame(columns=['item', 'QID', 'itemLabel', 'itemcountryLabel', 'industries', 'subsidiary', \\\n",
        "                               'sQID', 'subsidiaryLabel', 'subsidiarycountryLabel', 'sindustries', 'proportion', \\\n",
        "                               'proportionofLabel', 'pointoftime', 'starttime', 'endtime'])\n",
        "  # Create an empty dataframe for owner of\n",
        "  oo_df = pd.DataFrame(columns=['item', 'QID', 'itemLabel', 'itemcountryLabel', 'industries', 'ownerof', \\\n",
        "                                'ooQID', 'ownerofLabel', 'ownerofcountryLabel', 'ooindustries', 'proportion', \\\n",
        "                                'proportionofLabel', 'pointoftime', 'starttime', 'endtime'])\n",
        "\n",
        "  # initialize a list to store the results of each run\n",
        "  results = {\"p_df\": [], \"ob_df\": [], \"s_df\": [], \"oo_df\": []}\n",
        "  # making sure only those QIDs in root_QIDs that starts with \"Q\"\n",
        "  root_QIDs = [item for item in ids if item.startswith(\"Q\")]\n",
        "  # Find the maximum number of levels\n",
        "  max_levels = max(levels)\n",
        "  # Iterate from 0 to max_levels-1\n",
        "  for j in range(max_levels):\n",
        "    # Iterate over each element of the levels tuple\n",
        "    for i in range(len(levels)):\n",
        "      # Check if j is less than the value of levels[i]\n",
        "      if j < levels[i]:\n",
        "          # Iterate over each element of the item list\n",
        "          if i == 0:\n",
        "            # making sure only those QIDs in root_QIDs that starts with \"Q\"\n",
        "            root_QIDs = [item for item in root_QIDs if item.startswith(\"Q\")]\n",
        "            ## HAS PARENT\n",
        "            # make a new parent QID list with only those that has parent\n",
        "            new_p_QIDs = []\n",
        "            for id in root_QIDs:\n",
        "              parent_count = has_parent(id)\n",
        "              if parent_count > 0:\n",
        "                new_p_QIDs.append(id)\n",
        "            ## PARENT INFO\n",
        "            # Check if new_p_QIDs is empty\n",
        "            # get the parent info from the new QID list and concat it with the p_df dataframe\n",
        "            for id in new_p_QIDs:\n",
        "              p_df = pd.concat([p_df, get_company_parent_info(id)], ignore_index=True)\n",
        "            # tidy up industries and proportions with function tidy_each_df()\n",
        "            p_df = tidy_each_df(p_df)\n",
        "          elif i == 1:\n",
        "            # making sure only those QIDs in root_QIDs that starts with \"Q\"\n",
        "            root_QIDs = [item for item in root_QIDs if item.startswith(\"Q\")]\n",
        "            ## HAS OWNED BY\n",
        "            # make a new ownedby QID list with only those companies that has ownedby\n",
        "            new_ob_QIDs = []\n",
        "            for id in root_QIDs:\n",
        "              ownedby_count = has_ownedby(id)\n",
        "              if ownedby_count > 0:\n",
        "                new_ob_QIDs.append(id)\n",
        "            ## GET OWNED BY INFO\n",
        "            for id in new_ob_QIDs:\n",
        "              ob_df = pd.concat([ob_df, get_company_ownedby_info(id)], ignore_index=True)\n",
        "            # tidy up industries and proportions with function tidy_each_df()\n",
        "            ob_df = tidy_each_df(ob_df)\n",
        "          elif i == 2:\n",
        "            # making sure only those QIDs in root_QIDs that starts with \"Q\"\n",
        "            root_QIDs = [item for item in root_QIDs if item.startswith(\"Q\")]\n",
        "            ## HAS SUBSIDIARY\n",
        "            # make a new subsidiary QID list with only those companies that has subsidiary\n",
        "            new_s_QIDs = []\n",
        "            for id in root_QIDs:\n",
        "              subsidiary_count = has_subsidiary(id)\n",
        "              if subsidiary_count > 0:\n",
        "                new_s_QIDs.append(id)\n",
        "            ## GET SUBSIDIARY INFO\n",
        "            # get the subsidiaries info from the new QID list and concat it with the s_df dataframe\n",
        "            for id in new_s_QIDs:\n",
        "              s_df = pd.concat([s_df, get_company_subsidiary_info(id)], ignore_index=True)\n",
        "            # tidy up industries and proportions with function tidy_each_df()\n",
        "            s_df = tidy_each_df(s_df)\n",
        "          elif i == 3:\n",
        "            # making sure only those QIDs in root_QIDs that starts with \"Q\"\n",
        "            root_QIDs = [item for item in root_QIDs if item.startswith(\"Q\")]\n",
        "            ## HAS OWNER OF\n",
        "            # make a new ownerof QID list with only those companies that has owner of\n",
        "            new_oo_QIDs = []\n",
        "            for id in root_QIDs:\n",
        "              ownerof_count = has_ownerof(id)\n",
        "              if ownerof_count > 0:\n",
        "                new_oo_QIDs.append(id)\n",
        "            ## GET OWNER OF INFO\n",
        "            # get the owner of info from the new QID list and concat it with the oo_df dataframe\n",
        "            for id in new_oo_QIDs:\n",
        "              oo_df = pd.concat([oo_df, get_company_ownerof_info(id)], ignore_index=True)\n",
        "            # tidy up industries and proportions with function tidy_each_df()\n",
        "            oo_df = tidy_each_df(oo_df)\n",
        "    # append results after each number of runs\n",
        "    results['p_df'].append(p_df)\n",
        "    results['ob_df'].append(ob_df)\n",
        "    results['s_df'].append(s_df)\n",
        "    results['oo_df'].append(oo_df)\n",
        "    ## JOIN NEW UNIQUE QIDS\n",
        "    #getting all the QIDs that will needs to be queried again\n",
        "    # Store dataframes in a list\n",
        "    df_list = [p_df, ob_df, s_df, oo_df]\n",
        "    # Store column names in a list\n",
        "    col_list = ['pQID', 'obQID', 'sQID', 'ooQID']\n",
        "    # Create an empty list to store the column values\n",
        "    val_list = []\n",
        "    # Loop over the dataframes and the column names and append the column values to the list\n",
        "    for df, col in zip(df_list, col_list):\n",
        "      val_list.append(df[col].to_list())\n",
        "    #collect the unique values from val_list\n",
        "    new_list = []\n",
        "    [new_list.append(x) for x in val_list if x not in new_list]\n",
        "    # Convert the list to a pandas series\n",
        "    original_series = pd.Series(new_list)\n",
        "    # Use the explode method to split the nested lists into separate rows\n",
        "    exploded_series = original_series.explode()\n",
        "    # Use the drop_duplicates method to remove any duplicate values\n",
        "    unique_series = exploded_series.drop_duplicates()\n",
        "    # Convert the series back to a list\n",
        "    new_QIDs = unique_series.tolist()\n",
        "    #clean new_QIDs from 'nan' string values\n",
        "    new_QIDs = [x for x in new_QIDs if str(x) != 'nan']\n",
        "    # get a list of the new unique QIDs\n",
        "    new1_QIDs = list(set(new_QIDs) - set(root_QIDs))\n",
        "    # keep only those QIDs that starts with \"Q\"\n",
        "    new2_QIDs = [item for item in new1_QIDs if item.startswith(\"Q\")]\n",
        "    #rename the new list of QIDs as root_QIDs\n",
        "    root_QIDs = new2_QIDs\n",
        "\n",
        "  # return the list of results\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG9-2IhDIdOC"
      },
      "source": [
        "## FUNCTION 3. visualise_b2b_network()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRtztRRNP93i"
      },
      "source": [
        "### 3.1 Prepare dataset: clean_join(p_df, ob_df, s_df, oo_df) with various sub-functions such as is_human(), get_description() etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJ0ae73xpP_a"
      },
      "outputs": [],
      "source": [
        "# Checking if entity is a human, then get description too\n",
        "def is_human(items):\n",
        "    # Defining a list of user agents to alternate\n",
        "    user_agents = [\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Safari/605.1.15\",\n",
        "        \"Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0\",\n",
        "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Mobile/15E148 Safari/604.1\"\n",
        "    ]\n",
        "    # Constructing the SPARQL query\n",
        "    query = f\"\"\" SELECT DISTINCT\n",
        "                                ?human\n",
        "                                (REPLACE(STR(?human), \"http://www.wikidata.org/entity/\", \"\") AS ?QID)\n",
        "                                ?humanLabel\n",
        "                                ?humanDescription\n",
        "      WHERE {{\n",
        "            VALUES ?human {{wd:{items}}} .\n",
        "            ?human wdt:P31/wdt:P279* wd:Q5 .\n",
        "            ?article schema:about ?human .\n",
        "            ?article schema:inLanguage \"en\" .\n",
        "            ?article schema:isPartOf <https://en.wikipedia.org/>.\n",
        "            SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "            FILTER (LANG (?humanDescription) = \"en\")\n",
        "            }}\n",
        "    \"\"\"\n",
        "    # Choosing a random user agent from the list\n",
        "    user_agent = random.choice(user_agents)\n",
        "    # Sending the request to the Wikidata endpoint with the user agent header\n",
        "    response = requests.get(\"https://query.wikidata.org/sparql\", params={\"query\": query, \"format\": \"json\"}, headers={\"User-Agent\": user_agent})\n",
        "    # Parsing the response as JSON\n",
        "    data = response.json()\n",
        "\n",
        "    # Initializing an empty dictionary to store the information\n",
        "    info = {}\n",
        "    # Looping through the results\n",
        "    for result in data[\"results\"][\"bindings\"]:\n",
        "        # Adding the company's QID to the dictionary if exists\n",
        "        if \"QID\" in result:\n",
        "            info[\"QID\"] = result[\"QID\"][\"value\"]\n",
        "        # Adding the company label to the dictionary if exists\n",
        "        if \"humanLabel\" in result:\n",
        "            info[\"name\"] = result[\"humanLabel\"][\"value\"]\n",
        "        # Adding the industry label to the dictionary if exists\n",
        "        if \"humanDescription\" in result:\n",
        "            info[\"descr\"] = result[\"humanDescription\"][\"value\"]\n",
        "    # Returning the dictionary of information\n",
        "    return info\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wt6NH9bypcjH"
      },
      "outputs": [],
      "source": [
        "# Getting description for the unknown items\n",
        "def get_description(items):\n",
        "    # Defining a list of user agents to alternate\n",
        "    user_agents = [\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Safari/605.1.15\",\n",
        "        \"Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0\",\n",
        "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Mobile/15E148 Safari/604.1\"\n",
        "    ]\n",
        "    # Constructing the SPARQL query\n",
        "    query = f\"\"\" SELECT DISTINCT\n",
        "                                ?item\n",
        "                                (REPLACE(STR(?item), \"http://www.wikidata.org/entity/\", \"\") AS ?QID)\n",
        "                                ?itemLabel\n",
        "                                ?itemDescription\n",
        "      WHERE {{ wd:{items} rdfs:label ?itemLabel .\n",
        "            OPTIONAL {{ wd:{items} schema:description ?itemDescription }} .\n",
        "            BIND (wd:{items} AS ?item) .\n",
        "            SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "            FILTER (LANG (?itemDescription) = \"en\")\n",
        "            }}\n",
        "    \"\"\"\n",
        "    # Choosing a random user agent from the list\n",
        "    user_agent = random.choice(user_agents)\n",
        "    # Sending the request to the Wikidata endpoint with the user agent header\n",
        "    response = requests.get(\"https://query.wikidata.org/sparql\", params={\"query\": query, \"format\": \"json\"}, headers={\"User-Agent\": user_agent})\n",
        "    # Parsing the response as JSON\n",
        "    data = response.json()\n",
        "\n",
        "    # Initializing an empty dictionary to store the information\n",
        "    info = {}\n",
        "    # Looping through the results\n",
        "    for result in data[\"results\"][\"bindings\"]:\n",
        "        # Adding the company's QID to the dictionary if exists\n",
        "        if \"QID\" in result:\n",
        "            info[\"QID\"] = result[\"QID\"][\"value\"]\n",
        "        # Adding the company label to the dictionary if exists\n",
        "        if \"itemLabel\" in result:\n",
        "            info[\"name\"] = result[\"itemLabel\"][\"value\"]\n",
        "        # Adding the industry label to the dictionary if exists\n",
        "        if \"itemDescription\" in result:\n",
        "            info[\"descr\"] = result[\"itemDescription\"][\"value\"]\n",
        "    # Returning the dictionary of information\n",
        "    return info\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_each_df(df):\n",
        "  import pandas as pd\n",
        "  # convert endtime column to string and filter out rows with valid dates\n",
        "  if 'endtime' in df.columns:\n",
        "    df['endtime'] = df['endtime'].astype(str)\n",
        "    df1 = df[~df['endtime'].str.contains('\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z')]\n",
        "  new_df =pd.DataFrame(columns=df.columns)\n",
        "  # create a set of unique QID and obQID pairs\n",
        "  qid1_qid2_pairs = set(zip(df1.iloc[:, 1], df1.iloc[:, 6]))\n",
        "  # loop over the unique pairs and select the last row for each pair\n",
        "  for qid1, qid2 in qid1_qid2_pairs:\n",
        "      temp_df = df1.loc[(df1.iloc[:, 1] == qid1) & (df1.iloc[:, 6] == qid2)]\n",
        "      temp_df = temp_df.drop_duplicates().reset_index(drop = True)\n",
        "      if len(temp_df) == 1:\n",
        "          new_row = temp_df.iloc[0]\n",
        "          new_df = pd.concat([new_df, new_row.to_frame().T])\n",
        "      if len(temp_df) >= 2:\n",
        "          # check if proportionofLabel column is in temp_df\n",
        "          if 'proportionofLabel' in temp_df:\n",
        "              # check if proportionofLabel column has any value\n",
        "              if any(temp_df['proportionofLabel']):\n",
        "                  # use list comprehension to filter rows with proportionofLabel equal to 'authorised capital'\n",
        "                  auth_cap_rows = [row for row in temp_df.itertuples() if row.proportionofLabel == 'authorised capital']\n",
        "                  if auth_cap_rows:\n",
        "                      # select the last row from the filtered list\n",
        "                      new_row = auth_cap_rows[-1]\n",
        "                      new_df = pd.concat([new_df, pd.Series(new_row._asdict()).to_frame().T])\n",
        "          else:\n",
        "              # return auth_cap_rows empty\n",
        "              auth_cap_rows = []\n",
        "          # use list comprehension to filter rows with pointoftime or starttime containing valid dates\n",
        "          date_rows = [row for row in temp_df.itertuples() if '\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z' in (row.pointoftime or row.starttime)]\n",
        "          if date_rows:\n",
        "              # use max function to find the latest date from the filtered list\n",
        "              latest_date = max(date_rows, key=lambda x: x.pointoftime or x.starttime)\n",
        "              new_row = latest_date\n",
        "              new_df = pd.concat([new_df, pd.Series(new_row._asdict()).to_frame().T])\n",
        "  # reset index and drop duplicates\n",
        "  new_df.reset_index(drop=True, inplace=True)\n",
        "  new_df = new_df.drop_duplicates().reset_index(drop = True)\n",
        "\n",
        "  from pandas.errors import SettingWithCopyWarning\n",
        "  import warnings\n",
        "  warnings.filterwarnings('ignore', category=SettingWithCopyWarning)\n",
        "  # add those relations that has an endtime to the new dataframes\n",
        "  if 'endtime' in df.columns:\n",
        "      df['endtime'] = df['endtime'].astype(str)\n",
        "      df2 = df[df['endtime'].str.contains('\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z')]\n",
        "      df2['proportion'] = 4\n",
        "      df2['endtime'] = df2['endtime'].str[:10]\n",
        "      df2 = df2.drop_duplicates()\n",
        "  new_df = pd.concat([new_df, df2]).reset_index(drop = True)\n",
        "\n",
        "  return new_df\n"
      ],
      "metadata": {
        "id": "AzS4v0cfOrP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_dupl_joined(new_df2):\n",
        "  ### remove the duplicates from the joined datasets\n",
        "  # Step 1. save the single parent-child pairs in the new3_df2\n",
        "  #         if there are 2 or more of the same parent-child pair, then \\\n",
        "  #         drop the ones within these that has blank p_proportion.\n",
        "  # Step 2. in a new list comprehension (temp_step2_df) save the ones that now became single parent-child.\n",
        "  #         if there are 2 or more and if one of the p_prop value is == 2, then \\\n",
        "  ##        drop this row with value == 2\n",
        "  #         Finally, if  there are 2 or more and their p_prop value is equal, then keep only the last\n",
        "  ##        (this will allow more values to pass through if they differ)\n",
        "  # STEP 1\n",
        "  # Use a set to store the unique cQID values\n",
        "  cQID_set = set(new_df2['cQID'])\n",
        "  # Use a generator expression to iterate over the pQID values\n",
        "  pQID_gen = (i for i in new_df2['pQID'])\n",
        "  # Use a list comprehension to filter the rows based on pQID and cQID values\n",
        "  temp_new3_df2 = [new_df2.loc[(new_df2['pQID'] == i) & (new_df2['cQID'] == j)]\n",
        "                  for i in pQID_gen for j in cQID_set]\n",
        "  # Use another list comprehension to process the filtered rows based on the\n",
        "  # length and p_proportion values\n",
        "  new3_df2 = pd.concat([row.iloc[0].to_frame().T if len(row) == 1\n",
        "                        else row.dropna(subset=['p_proportion']).loc[row['p_proportion'].notnull()]\n",
        "                        for row in temp_new3_df2])\n",
        "  new3_df2 = new3_df2.drop_duplicates().reset_index(drop = True)\n",
        "  # STEP 2\n",
        "  # Use a set to store the unique cQID values\n",
        "  cQID_set2 = set(new3_df2['cQID'])\n",
        "  # Use a generator expression to iterate over the pQID values\n",
        "  pQID_gen2 = (i for i in new3_df2['pQID'])\n",
        "  # Use a list comprehension to filter the rows based on pQID and cQID values\n",
        "  temp_step2_df = [new3_df2.loc[(new3_df2['pQID'] == i) & (new3_df2['cQID'] == j)]\n",
        "                  for i in pQID_gen2 for j in cQID_set2]\n",
        "  # Drop the rows with p_proportion value equal to 2\n",
        "  final_df = pd.concat([row.iloc[0].to_frame().T if len(row) == 1 \\\n",
        "                        else row.loc[row['p_proportion'] != 2] for row in temp_step2_df])\n",
        "  # Keep only the last row if there are multiple rows with the same pQID and cQID\n",
        "  # values and equal p_proportion values\n",
        "  final_df = final_df.drop_duplicates(subset=['pQID', 'cQID', 'p_proportion'], keep='last')\n",
        "  final_df = final_df.drop_duplicates().reset_index(drop = True)\n",
        "  # fill the blank p_proportion values with number 3, this will mean unknown on the graph\n",
        "  final_df['p_proportion'] = final_df['p_proportion'].fillna(3)\n",
        "  return final_df"
      ],
      "metadata": {
        "id": "YH2zLB4PXd-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_industries(final_df):\n",
        "  import numpy as np\n",
        "\n",
        "  # create a dictionary of industry frequencies\n",
        "  # split industry columns by \";\" and create separate columns\n",
        "  df_split1 = final_df[\"p_industries\"].str.split(\";\", expand=True)\n",
        "  df_split2 = final_df[\"c_industries\"].str.split(\";\", expand=True)\n",
        "  # join the two dataframes horizontally\n",
        "  df_split = pd.concat([df_split1, df_split2], axis=1)\n",
        "  # reshape the dataframe so that each expression is in a single column\n",
        "  df_melt = df_split.melt(value_name=\"expression\")\n",
        "  # drop the variable column\n",
        "  df_melt = df_melt.drop(\"variable\", axis=1)\n",
        "  # count the frequency of each expression and convert to dictionary\n",
        "  freq = df_melt[\"expression\"].value_counts().to_dict()\n",
        "\n",
        "  # define a custom function to pick the expression with highest frequency\n",
        "  def pick_expression(lst):\n",
        "    import numpy as np\n",
        "    # split the list by semicolon\n",
        "    lst = lst.split(\";\")\n",
        "    # initialize the best expression and frequency\n",
        "    best_exp = None\n",
        "    best_freq = 0\n",
        "    # loop through the list\n",
        "    for exp in lst:\n",
        "        # get the frequency of the expression\n",
        "        freq_exp = freq.get(exp, 0)\n",
        "        # if the frequency is higher than the best frequency, update the best expression and frequency\n",
        "        if freq_exp > best_freq:\n",
        "            best_exp = exp\n",
        "            best_freq = freq_exp\n",
        "    # return the best expression\n",
        "    return best_exp\n",
        "\n",
        "  # apply the custom function to each cell in col2\n",
        "  final_df[\"top_p_industries\"] = final_df[\"p_industries\"].apply(pick_expression)\n",
        "  final_df[\"top_c_industries\"] = final_df[\"c_industries\"].apply(pick_expression)\n",
        "\n",
        "  # replace the empty strings with NaN\n",
        "  final_df[\"top_p_industries\"] = final_df[\"top_p_industries\"].replace(\"\", np.nan)\n",
        "  final_df[\"top_c_industries\"] = final_df[\"top_c_industries\"].replace(\"\", np.nan)\n",
        "  final_df[\"p_industries\"] = final_df[\"p_industries\"].replace(\"\", np.nan)\n",
        "  final_df[\"c_industries\"] = final_df[\"c_industries\"].replace(\"\", np.nan)\n",
        "\n",
        "  # use isna method to get a boolean mask of missing values\n",
        "  mask = final_df[\"top_p_industries\"].isna()\n",
        "  mask2 = final_df[\"top_c_industries\"].isna()\n",
        "  mask3 = final_df[\"p_industries\"].isna()\n",
        "  mask4 = final_df[\"c_industries\"].isna()\n",
        "\n",
        "  # use loc method to assign \"unknown\" to the rows where col2 is missing\n",
        "  final_df.loc[mask, \"top_p_industries\"] = \"unknown\"\n",
        "  final_df.loc[mask2, \"top_c_industries\"] = \"unknown\"\n",
        "  final_df.loc[mask3, \"p_industries\"] = \"unknown\"\n",
        "  final_df.loc[mask4, \"c_industries\"] = \"unknown\"\n",
        "\n",
        "  final_df[\"top_p_industries\"] = final_df[\"top_p_industries\"].str.lstrip()\n",
        "  final_df[\"top_c_industries\"] = final_df[\"top_c_industries\"].str.lstrip()\n",
        "\n",
        "  # make a list of all those parent QIDs where the industry is \"unknown\"\n",
        "  unique_values1 = final_df.loc[(final_df['top_p_industries'] == 'unknown'), 'pQID'].drop_duplicates().values.tolist()\n",
        "  # making sure only those QIDs in unique_values that starts with \"Q\"\n",
        "  unique_values1 = [item for item in unique_values1 if item.startswith(\"Q\")]\n",
        "  # create a dataframe for the results\n",
        "  p_human_df1 = pd.DataFrame(columns=[\"QID\", \"name\", \"descr\"])\n",
        "  #initialising a new list to store the results\n",
        "  results =[]\n",
        "  # Loop through the values\n",
        "  for id in unique_values1:\n",
        "    # Run the module with the value as input and get the result\n",
        "    result = pd.DataFrame(is_human(id), index=[0])\n",
        "    # Appending the new dataframe to the list\n",
        "    results.append(result)\n",
        "\n",
        "  # join the new results to the dataframe\n",
        "  p_human_df1 = pd.concat([p_human_df1] + results, axis=0, join='outer').drop_duplicates().reset_index(drop = True)\n",
        "\n",
        "  #apply humans and their descriptions to new_df2 dataframe\n",
        "  for index, row in final_df.iterrows():\n",
        "      if row['pQID'] in p_human_df1['QID'].values:\n",
        "          # get the index of the matching row in p_human_df1\n",
        "          match_index = p_human_df1[p_human_df1['QID'] == row['pQID']].index\n",
        "          # get the value from the \"descr\" column of p_human_df1\n",
        "          descr_value = p_human_df1.loc[match_index[0], 'descr']\n",
        "          # update the values in new_df2\n",
        "          final_df.loc[index, 'p_industries'] = descr_value\n",
        "          final_df.loc[index, 'top_p_industries'] = \"human\"\n",
        "\n",
        "  # make a list of all those QIDs where the industry is \"unknown\"\n",
        "  unique_values3 = final_df.loc[(final_df['top_p_industries'] == 'unknown'), 'pQID'].drop_duplicates().values.tolist()\n",
        "  unique_values4 = final_df.loc[(final_df['top_c_industries'] == 'unknown'), 'cQID'].drop_duplicates().values.tolist()\n",
        "  # making sure only those QIDs in unique_values that starts with \"Q\"\n",
        "  unique_values3 = [item for item in unique_values3 if item.startswith(\"Q\")]\n",
        "  unique_values4 = [item for item in unique_values4 if item.startswith(\"Q\")]\n",
        "  # create a dataframe for the results\n",
        "  p_item_df1 = pd.DataFrame(columns=[\"QID\", \"name\", \"descr\"])\n",
        "  #initialising a new list to store the results\n",
        "  results =[]\n",
        "  # Loop through the values\n",
        "  for id in unique_values3:\n",
        "    # Run the module with the value as input and get the result\n",
        "    result = pd.DataFrame(get_description(id), index=[0])\n",
        "    # Appending the new dataframe to the list\n",
        "    results.append(result)\n",
        "\n",
        "  # join the new results to the dataframe\n",
        "  p_item_df1 = pd.concat([p_item_df1] + results, axis=0, join='outer').drop_duplicates().reset_index(drop = True)\n",
        "\n",
        "  # create a dataframe for the results\n",
        "  c_item_df2 = pd.DataFrame(columns=[\"QID\", \"name\", \"descr\"])\n",
        "  #initialising a new list to store the results\n",
        "  results2 =[]\n",
        "  # Loop through the values\n",
        "  for id2 in unique_values4:\n",
        "    # Run the module with the value as input and get the result\n",
        "    result2 = pd.DataFrame(get_description(id2), index=[0])\n",
        "    # Appending the new dataframe to the list\n",
        "    results2.append(result2)\n",
        "\n",
        "  # join the new results to the dataframe\n",
        "  c_item_df2 = pd.concat([c_item_df2] + results2, axis=0, join='outer').drop_duplicates().reset_index(drop = True)\n",
        "\n",
        "  #apply descriptions for unknown parents to new_df2 dataframe\n",
        "  for index, row in final_df.iterrows():\n",
        "      if row['pQID'] in p_item_df1['QID'].values:\n",
        "          # get the index of the matching row in p_human_df1\n",
        "          match_index = p_item_df1[p_item_df1['QID'] == row['pQID']].index\n",
        "          # get the value from the \"descr\" column of p_human_df1\n",
        "          descr_value = p_item_df1.loc[match_index[0], 'descr']\n",
        "          # update the values in new_df2\n",
        "          final_df.loc[index, 'p_industries'] = descr_value\n",
        "          final_df.loc[index, 'top_p_industries'] = \"other\"\n",
        "\n",
        "  #apply descriptions for unknown children to new_df2 dataframe\n",
        "  for index, row in final_df.iterrows():\n",
        "      if row['cQID'] in c_item_df2['QID'].values:\n",
        "          # get the index of the matching row in p_human_df1\n",
        "          match_index2 = c_item_df2[c_item_df2['QID'] == row['cQID']].index\n",
        "          # get the value from the \"descr\" column of p_human_df1\n",
        "          descr_value2 = c_item_df2.loc[match_index2[0], 'descr']\n",
        "          # update the values in new_df2\n",
        "          final_df.loc[index, 'c_industries'] = descr_value2\n",
        "          final_df.loc[index, 'top_c_industries'] = \"other\"\n",
        "\n",
        "  # convert the values in columns to strings\n",
        "  final_df['p_industries'] = final_df['p_industries'].astype(str)\n",
        "  final_df['top_p_industries'] = final_df['top_p_industries'].astype(str)\n",
        "  final_df['c_industries'] = final_df['c_industries'].astype(str)\n",
        "  final_df['top_c_industries'] = final_df['top_c_industries'].astype(str)\n",
        "  return final_df"
      ],
      "metadata": {
        "id": "FwrR8XelzIQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a function to convert the known country names to the continents they belong\n",
        "# should first install pycountry and pycountry_convert\n",
        "def country_to_continent(country_name):\n",
        "  import pycountry_convert as pc\n",
        "  if country_name == 'unknown':\n",
        "    return 'unknown'\n",
        "  else:\n",
        "    country_alpha2 = pc.country_name_to_country_alpha2(country_name)\n",
        "    country_continent_code = pc.country_alpha2_to_continent_code(country_alpha2)\n",
        "    country_continent_name = pc.convert_continent_code_to_continent_name(country_continent_code)\n",
        "    return country_continent_name\n"
      ],
      "metadata": {
        "id": "z-8KWbNPM6S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEIDYYq5XeZp"
      },
      "outputs": [],
      "source": [
        "# clean each dataset and then join them\n",
        "def clean_join(p_df, ob_df, s_df, oo_df):\n",
        "  #global final_df, new_p_df, new_ob_df, new_s_df, new_oo_df\n",
        "  # convert the data columns to strings\n",
        "  p_df.loc[:, ['pointoftime', 'starttime']] = p_df.loc[:, ['pointoftime', 'starttime']].astype(str)\n",
        "  ob_df.loc[:, ['pointoftime', 'starttime']] = ob_df.loc[:, ['pointoftime', 'starttime']].astype(str)\n",
        "  s_df.loc[:, ['pointoftime', 'starttime']] = s_df.loc[:, ['pointoftime', 'starttime']].astype(str)\n",
        "  oo_df.loc[:, ['pointoftime', 'starttime']] = oo_df.loc[:, ['pointoftime', 'starttime']].astype(str)\n",
        "  ### remove the duplicates within p dataset - keep the last\n",
        "  new_p_df = pd.DataFrame(columns=['item', 'QID', 'itemLabel', 'itemcountryLabel', 'industries', 'parent', \\\n",
        "                                    'pQID', 'parentLabel', 'parentcountryLabel', 'pindustries', 'proportion', \\\n",
        "                                    'proportionofLabel', 'pointoftime', 'starttime', 'endtime'])\n",
        "  new_p_df = pd.concat([new_p_df, clear_each_df(p_df)]).reset_index(drop=True)\n",
        "\n",
        "  ### remove the duplicates within ob dataset - keep the last\n",
        "  new_ob_df = pd.DataFrame(columns=['item', 'QID', 'itemLabel', 'itemcountryLabel', 'industries', 'ownedby', \\\n",
        "                                    'obQID', 'ownedbyLabel', 'ownedbycountryLabel', 'obindustries', 'proportion', \\\n",
        "                                    'proportionofLabel', 'pointoftime', 'starttime', 'endtime'])\n",
        "  new_ob_df = pd.concat([new_ob_df, clear_each_df(ob_df)]).reset_index(drop=True)\n",
        "\n",
        "  ### remove the duplicates within s dataset - keep the last\n",
        "  new_s_df = pd.DataFrame(columns=['item', 'QID', 'itemLabel', 'itemcountryLabel', 'industries', 'subsidiary', \\\n",
        "                                    'sQID', 'subsidiaryLabel', 'subsidiarycountryLabel', 'sindustries', 'proportion', \\\n",
        "                                    'proportionofLabel', 'pointoftime', 'starttime', 'endtime'])\n",
        "  new_s_df = pd.concat([new_s_df, clear_each_df(s_df)]).reset_index(drop=True)\n",
        "\n",
        "  ### remove the duplicates within oo dataset - keep the last\n",
        "  new_oo_df = pd.DataFrame(columns=['item', 'QID', 'itemLabel', 'itemcountryLabel', 'industries', 'ownerof', \\\n",
        "                                    'ooQID', 'ownerofLabel', 'ownerofcountryLabel', 'ooindustries', 'proportion', \\\n",
        "                                    'proportionofLabel', 'pointoftime', 'starttime', 'endtime'])\n",
        "  new_oo_df = pd.concat([new_oo_df, clear_each_df(oo_df)]).reset_index(drop=True)\n",
        "\n",
        "  ### join together the dataframes into a new_df\n",
        "  new_df = pd.DataFrame(columns=[\"pQID\", \"parent\", \"parent_country\", \"p_industries\", \"cQID\", \"child\", \\\n",
        "                                \"child_country\", \"c_industries\", \"p_proportion\", \"proportionofLabel\", \\\n",
        "                                \"pointoftime\", \"starttime\", \"endtime\"])\n",
        "  # Create an empty list to store the new rows\n",
        "  new_p_rows = []\n",
        "  # Loop over the new_p_df dataframe\n",
        "  for i in range(len(new_p_df)):\n",
        "      # Create a new row as a dictionary\n",
        "      new_p_row = {'pQID': new_p_df['pQID'][i], 'parent': new_p_df['parentLabel'][i], 'parent_country': new_p_df['parentcountryLabel'][i], \\\n",
        "                'p_industries': new_p_df['pindustries'][i], 'cQID': new_p_df['QID'][i], \\\n",
        "                'child': new_p_df['itemLabel'][i], 'child_country': new_p_df['itemcountryLabel'][i], 'c_industries': new_p_df['industries'][i], \\\n",
        "                'p_proportion':new_p_df['proportion'][i], 'proportionofLabel':new_p_df['proportionofLabel'][i], \\\n",
        "                'pointoftime':new_p_df['pointoftime'][i], 'starttime':new_p_df['starttime'][i], 'endtime': new_p_df['endtime'][i]}\n",
        "      # Convert the dictionary to a dataframe and append it to the list\n",
        "      new_p_rows.append(pd.DataFrame(new_p_row, index=[0]))\n",
        "  # Concatenate the list of dataframes with the original dataframe\n",
        "  new_df = pd.concat([new_df] + new_p_rows, ignore_index=True)\n",
        "  new_df['p_proportion'] = new_df['p_proportion'].fillna(2)\n",
        "  # Create an empty list to store the new rows\n",
        "  new_ob_rows = []\n",
        "  for i in range(len(new_ob_df)):\n",
        "      new_ob_row = {'pQID': new_ob_df['obQID'][i], 'parent': new_ob_df['ownedbyLabel'][i], 'parent_country': new_ob_df['ownedbycountryLabel'][i], \\\n",
        "                'p_industries': new_ob_df['obindustries'][i], 'cQID': new_ob_df['QID'][i], \\\n",
        "                'child': new_ob_df['itemLabel'][i], 'child_country': new_ob_df['itemcountryLabel'][i], 'c_industries': new_ob_df['industries'][i], \\\n",
        "                'p_proportion':new_ob_df['proportion'][i], 'proportionofLabel':new_ob_df['proportionofLabel'][i], \\\n",
        "                'pointoftime':new_ob_df['pointoftime'][i], 'starttime':new_ob_df['starttime'][i], 'endtime': new_ob_df['endtime'][i]}\n",
        "      new_ob_rows.append(pd.DataFrame(new_ob_row, index=[0]))\n",
        "  # Concatenate the list of dataframes with the original dataframe\n",
        "  new_df = pd.concat([new_df] + new_ob_rows, ignore_index=True)\n",
        "  # Create an empty list to store the new rows\n",
        "  new_s_rows = []\n",
        "  for i in range(len(new_s_df)):\n",
        "      new_s_row = {'pQID': new_s_df['QID'][i], 'parent': new_s_df['itemLabel'][i], 'parent_country': new_s_df['itemcountryLabel'][i], \\\n",
        "                'p_industries': new_s_df['industries'][i], 'cQID': new_s_df['sQID'][i], \\\n",
        "                'child': new_s_df['subsidiaryLabel'][i], 'child_country': new_s_df['subsidiarycountryLabel'][i], 'c_industries': new_s_df['sindustries'][i], \\\n",
        "                'p_proportion':new_s_df['proportion'][i], 'proportionofLabel':new_s_df['proportionofLabel'][i], \\\n",
        "                'pointoftime':new_s_df['pointoftime'][i], 'starttime':new_s_df['starttime'][i], 'endtime': new_s_df['endtime'][i]}\n",
        "      new_s_rows.append(pd.DataFrame(new_s_row, index=[0]))\n",
        "  # Concatenate the list of dataframes with the original dataframe\n",
        "  new_df = pd.concat([new_df] + new_s_rows, ignore_index=True)\n",
        "  # Create an empty list to store the new rows\n",
        "  new_oo_rows = []\n",
        "  for i in range(len(new_oo_df)):\n",
        "      new_oo_row = {'pQID': new_oo_df['QID'][i], 'parent': new_oo_df['itemLabel'][i], 'parent_country': new_oo_df['itemcountryLabel'][i], \\\n",
        "                'p_industries': new_oo_df['industries'][i], 'cQID': new_oo_df['ooQID'][i], \\\n",
        "                'child': new_oo_df['ownerofLabel'][i], 'child_country': new_oo_df['ownerofcountryLabel'][i], 'c_industries': new_oo_df['ooindustries'][i], \\\n",
        "                'p_proportion':new_oo_df['proportion'][i], 'proportionofLabel':new_oo_df['proportionofLabel'][i], \\\n",
        "                'pointoftime':new_oo_df['pointoftime'][i], 'starttime':new_oo_df['starttime'][i], 'endtime': new_oo_df['endtime'][i]}\n",
        "      new_oo_rows.append(pd.DataFrame(new_oo_row, index=[0]))\n",
        "  # Concatenate the list of dataframes with the original dataframe\n",
        "  new_df = pd.concat([new_df] + new_oo_rows, ignore_index=True)\n",
        "  new_df2 = new_df.drop_duplicates().reset_index(drop = True)\n",
        "\n",
        "  # drop the duplicates from the joined dataframe with various rules defined by drop_dupl_joined() function\n",
        "  final_df = drop_dupl_joined(new_df2)\n",
        "  # clear up industries and find the top ones for classification. unknown ones: get description (also checks if its human)\n",
        "  final_df = top_industries(final_df)\n",
        "\n",
        "  # replace missing country values with \"unknown\"\n",
        "  final_df[\"parent_country\"] = final_df[\"parent_country\"].replace(\"\", np.nan)\n",
        "  final_df[\"child_country\"] = final_df[\"child_country\"].replace(\"\", np.nan)\n",
        "  mask_p = final_df[\"parent_country\"].isna()\n",
        "  mask_c = final_df[\"child_country\"].isna()\n",
        "  final_df.loc[mask_p, \"parent_country\"] = \"unknown\"\n",
        "  final_df.loc[mask_c, \"child_country\"] = \"unknown\"\n",
        "  # apply the country_to_continent function to get the continents the companies are located\n",
        "  final_df['parent_continent'] = final_df['parent_country'].apply(country_to_continent)\n",
        "  final_df['child_continent'] = final_df['child_country'].apply(country_to_continent)\n",
        "  return final_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn2ym2t8hnxV"
      },
      "source": [
        "### Function 3.2: visualise_b2b_network(final_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bf7r3dPYsuFU"
      },
      "outputs": [],
      "source": [
        "def visualise_b2b_network(final_df):\n",
        "  # import packages\n",
        "  import pandas as pd\n",
        "  from pyvis.network import Network\n",
        "  import seaborn as sns\n",
        "  import colorsys\n",
        "  from collections import Counter\n",
        "  import json\n",
        "\n",
        "  # create network object\n",
        "  net = Network(\n",
        "      notebook = True,\n",
        "      directed = True,            # directed graph\n",
        "      bgcolor = \"snow\",          # background color of graph\n",
        "      font_color = \"navy\",        # use navy for node labels\n",
        "      cdn_resources = 'in_line',  # make sure Jupyter notebook can display correctly\n",
        "      height = \"1000px\",          # height of chart\n",
        "      width = \"100%\",             # fill the entire width\n",
        "      select_menu=True,           # user can choose from campany name list\n",
        "      filter_menu=True,           # user can search for colors - indicators for industries\n",
        "      neighborhood_highlight=True, # clicking on a node highlights its connections and grays out others\n",
        "      )\n",
        "\n",
        "  # create lists of nodes and edges from dataframe columns\n",
        "  nodes = []\n",
        "  edges = []\n",
        "  for parent, child, p_industry, c_industry, top_p_industry, top_c_industry, parent_country, parent_continent, \\\n",
        "      child_country, child_continent, proportion, endtime in zip(final_df['parent'], \\\n",
        "                                                                 final_df['child'], \\\n",
        "                                                                 final_df['p_industries'], \\\n",
        "                                                                 final_df['c_industries'], \\\n",
        "                                                                 final_df['top_p_industries'], \\\n",
        "                                                                 final_df['top_c_industries'], \\\n",
        "                                                                 final_df['parent_country'], \\\n",
        "                                                                 final_df['parent_continent'], \\\n",
        "                                                                 final_df['child_country'], \\\n",
        "                                                                 final_df['child_continent'], \\\n",
        "                                                                 final_df['p_proportion'], \\\n",
        "                                                                 final_df['endtime']):\n",
        "      nodes.append((parent, p_industry, top_p_industry, parent_country, parent_continent))\n",
        "      nodes.append((child, c_industry, top_c_industry, child_country, child_continent))\n",
        "      edges.append((parent, child, proportion, endtime))\n",
        "\n",
        "  # remove duplicate nodes\n",
        "  #nodes = list(set(nodes))\n",
        "\n",
        "  # create a set of unique industries\n",
        "  industries = set(final_df['top_p_industries']).union(set(final_df['top_c_industries']))\n",
        "\n",
        "  # get the number of unique industries\n",
        "  n_colors = len(industries)\n",
        "\n",
        "  # create a dictionary that maps each unique industry to a pastel color\n",
        "  pastel_colors = sns.color_palette('muted', n_colors)\n",
        "\n",
        "  # define a function that converts an rgb tuple to an rgba string with a pastel factor\n",
        "  def rgb_to_rgba(rgb, pastel_factor):\n",
        "      # convert rgb values to hls values\n",
        "      h, l, s = colorsys.rgb_to_hls(*rgb)\n",
        "      # increase lightness by multiplying with pastel factor\n",
        "      l *= pastel_factor\n",
        "      # convert hls values back to rgb values\n",
        "      r, g, b = colorsys.hls_to_rgb(h, l, s)\n",
        "      # normalize rgb values to be between 0 and 255\n",
        "      r = int(r * 255)\n",
        "      g = int(g * 255)\n",
        "      b = int(b * 255)\n",
        "      # add alpha value of 0.7\n",
        "      a = 0.7\n",
        "      # format rgba values as a string\n",
        "      rgba = f'rgba({r}, {g}, {b}, {a})'\n",
        "      return rgba\n",
        "\n",
        "  group_color_map = {}\n",
        "  for i, top_industry in enumerate(industries):\n",
        "      # use the function with pastel_colors[i] and a pastel factor of 1.2\n",
        "      group_color_map[top_industry] = rgb_to_rgba(pastel_colors[i], 1.2)\n",
        "\n",
        "  # create a dictionary that maps each node to its degree\n",
        "  node_degree_map = Counter()\n",
        "  for edge in edges:\n",
        "      node_degree_map[edge[0]] += 1\n",
        "      node_degree_map[edge[1]] += 1\n",
        "\n",
        "  # define a function that takes a degree and a scaling factor and returns a size for the node\n",
        "  def degree_to_size(degree, scaling_factor):\n",
        "      # multiply degree by scaling factor to get base size\n",
        "      base_size = degree * scaling_factor\n",
        "      # add minimum size to base size to ensure no node is too small\n",
        "      min_size = 3\n",
        "      final_size = base_size + min_size\n",
        "      # return final size as an integer\n",
        "      return int(final_size)\n",
        "\n",
        "  # create a dictionary that maps each node to its size\n",
        "  node_size_map = {}\n",
        "  for node in node_degree_map:\n",
        "      # use the function with node_degree_map[node] and a scaling factor of 5\n",
        "      node_size_map[node] = degree_to_size(node_degree_map[node], 5)\n",
        "\n",
        "  # define a different color for the highlighted nodes\n",
        "  highlight_color = 'green'\n",
        "  #node1 = root_companies[0]\n",
        "  #node2 = root_companies[1]\n",
        "\n",
        "  # create a set of unique countries and continents\n",
        "  countries = set(final_df['parent_country']).union(set(final_df['child_country']))\n",
        "  continents = set(final_df['parent_continent']).union(set(final_df['child_continent']))\n",
        "\n",
        "  # add nodes and edges to network object\n",
        "  for node, industry, top_industry, countries, continents in nodes:\n",
        "      color = group_color_map[top_industry]\n",
        "      # check if the node name is equal to node1 or node2 and use highlight_color if so\n",
        "      #if node == root_companies:\n",
        "      #    color = highlight_color\n",
        "      #else:\n",
        "      #    color = group_color_map[top_industry]\n",
        "      # assign shapes for teh continents\n",
        "      if continents == 'North America':\n",
        "        node_shape = 'triangle'\n",
        "      elif continents == 'South America':\n",
        "        node_shape = 'triangleDown'\n",
        "      elif continents == 'Europe':\n",
        "        node_shape = 'star'\n",
        "      elif continents == 'Africa':\n",
        "        node_shape = 'diamond'\n",
        "      elif continents == 'Asia':\n",
        "        node_shape = 'square'\n",
        "      elif continents == 'Australia':\n",
        "        node_shape = 'ellipse'\n",
        "      else:\n",
        "        node_shape = 'dot'\n",
        "      size = node_size_map[node]\n",
        "      net.add_node(node, label=node, group=top_industry, color=color, shape=node_shape, title=node + ', '+ countries + ' (' + industry + ')', value=size)\n",
        "\n",
        "  for e in edges:\n",
        "      if e[2] > 0 and e[2] < 0.5:\n",
        "          net.add_edge(e[0], e[1], title=str(e[2]), arrows={\"to\": True}, color='turquoise') #value=(e[2]/100), dashes=[6,10,1,10]\n",
        "      elif e[2] >= 0.5 and e[2] <= 1:\n",
        "          net.add_edge(e[0], e[1], title=str(e[2]), arrows={\"to\": True}, color='violet') #value=(e[2]/10),  dashes=False\n",
        "      elif e[2] == 2:\n",
        "          net.add_edge(e[0], e[1], title=\"parent\", arrows={\"to\": True}, color='salmon') #dashes=[5,5]\n",
        "      elif e[2] == 3:\n",
        "          net.add_edge(e[0], e[1], title=\"unknown value\", arrows={\"to\": True}, color='lime') # dashes=[1,10]\n",
        "      elif e[2] == 4:\n",
        "          net.add_edge(e[0], e[1], title=\"end date: \"+str(e[3]), arrows={\"to\": True}, color='grey')\n",
        "\n",
        "\n",
        "  net.repulsion(\n",
        "      node_distance=100,\n",
        "      central_gravity=0.2,\n",
        "      spring_length=200,\n",
        "      spring_strength=0.05,\n",
        "      damping=0.09,\n",
        "  )\n",
        "\n",
        "  net.show_buttons(filter_='physics')\n",
        "  #net.show_buttons(filter_=['nodes', 'edges', 'physics'])\n",
        "\n",
        "  # show network graph\n",
        "  return net.show('B2B_network_Wiki.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MIT License\n",
        "Copyright (c) 2023 Zsofia Baruwa"
      ],
      "metadata": {
        "id": "UcRIPxvcJzEb"
      }
    }
  ]
}
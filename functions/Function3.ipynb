{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-KhB4NAsH975",
        "aRtztRRNP93i",
        "bAq56Sgx9BND",
        "Sn2ym2t8hnxV"
      ],
      "authorship_tag": "ABX9TyNc6kJK5Ij39Jd8s3iJsMje",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zb15/B2BNetworkWiki/blob/main/functions/Function3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG9-2IhDIdOC"
      },
      "source": [
        "## FUNCTION 3. visualise_b2b_network()"
      ]
    },
    {
    "cell_type": "markdown",
    "source": [
      "Currently the package has 3 main functions:            \n",
      "\n",
      "1.   choose_company(input_name, search_option='all')\n",
      "2.   get_companies_network(QIDs, num_runs=(5,5,5,5))\n",
      "3.   visualise_b2b_network(df)\n",
      "\n",
      "The **visualise_b2b_network()** function has 2 steps at the moment (that can easily be made into 1 function).\n",
      "The 1st step (with clean_and_join(p_df, ob_df, s_df, oo_df) function) is to clean the 4 datasets and join them\n",
      "together into a final_df. The wikidata requires a lot of cleaning, handling duplicates etc. and comments are included\n",
      "within (the quite long) code. And the function that visualising the network from final_df dataframe (visualise_b2b_network()).\n"
    ],
    "metadata": {
      "id": "pzWksCEx3Gsr"
    }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KhB4NAsH975"
      },
      "source": [
        "### importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMYRKsCaG1Pc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e0d7b46-6b5c-4cde-d582-ff54b6115971"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install pyvis --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycountry --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjorEeQLXDki",
        "outputId": "c59447e1-4f77-4c82-f027-6ef5bce7afb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pycountry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycountry-convert --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRWAJ0jxYmM7",
        "outputId": "8311b595-3886-4769-d979-36f29f059982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/228.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.7/228.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the modules\n",
        "import requests\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "JT95kImro9Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRtztRRNP93i"
      },
      "source": [
        "### Prepare dataset: including various functions such as is_human(), get_description() etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJ0ae73xpP_a"
      },
      "outputs": [],
      "source": [
        "# Checking if entity is a human, then get description too\n",
        "def is_human(items):\n",
        "    # Defining a list of user agents to alternate\n",
        "    user_agents = [\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Safari/605.1.15\",\n",
        "        \"Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0\",\n",
        "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Mobile/15E148 Safari/604.1\"\n",
        "    ]\n",
        "    # Constructing the SPARQL query\n",
        "    query = f\"\"\" SELECT DISTINCT\n",
        "                                ?human\n",
        "                                (REPLACE(STR(?human), \"http://www.wikidata.org/entity/\", \"\") AS ?QID)\n",
        "                                ?humanLabel\n",
        "                                ?humanDescription\n",
        "      WHERE {{\n",
        "            VALUES ?human {{wd:{items}}} .\n",
        "            ?human wdt:P31/wdt:P279* wd:Q5 .\n",
        "            ?article schema:about ?human .\n",
        "            ?article schema:inLanguage \"en\" .\n",
        "            ?article schema:isPartOf <https://en.wikipedia.org/>.\n",
        "            SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "            FILTER (LANG (?humanDescription) = \"en\")\n",
        "            }}\n",
        "    \"\"\"\n",
        "    # Choosing a random user agent from the list\n",
        "    user_agent = random.choice(user_agents)\n",
        "    # Sending the request to the Wikidata endpoint with the user agent header\n",
        "    response = requests.get(\"https://query.wikidata.org/sparql\", params={\"query\": query, \"format\": \"json\"}, headers={\"User-Agent\": user_agent})\n",
        "    # Parsing the response as JSON\n",
        "    data = response.json()\n",
        "\n",
        "    # Initializing an empty dictionary to store the information\n",
        "    info = {}\n",
        "    # Looping through the results\n",
        "    for result in data[\"results\"][\"bindings\"]:\n",
        "        # Adding the company's QID to the dictionary if exists\n",
        "        if \"QID\" in result:\n",
        "            info[\"QID\"] = result[\"QID\"][\"value\"]\n",
        "        # Adding the company label to the dictionary if exists\n",
        "        if \"humanLabel\" in result:\n",
        "            info[\"name\"] = result[\"humanLabel\"][\"value\"]\n",
        "        # Adding the industry label to the dictionary if exists\n",
        "        if \"humanDescription\" in result:\n",
        "            info[\"descr\"] = result[\"humanDescription\"][\"value\"]\n",
        "    # Returning the dictionary of information\n",
        "    return info\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wt6NH9bypcjH"
      },
      "outputs": [],
      "source": [
        "# Getting description for the unknown items\n",
        "def get_description(items):\n",
        "    # Defining a list of user agents to alternate\n",
        "    user_agents = [\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Safari/605.1.15\",\n",
        "        \"Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0\",\n",
        "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Mobile/15E148 Safari/604.1\"\n",
        "    ]\n",
        "    # Constructing the SPARQL query\n",
        "    query = f\"\"\" SELECT DISTINCT\n",
        "                                ?item\n",
        "                                (REPLACE(STR(?item), \"http://www.wikidata.org/entity/\", \"\") AS ?QID)\n",
        "                                ?itemLabel\n",
        "                                ?itemDescription\n",
        "      WHERE {{ wd:{items} rdfs:label ?itemLabel .\n",
        "            OPTIONAL {{ wd:{items} schema:description ?itemDescription }} .\n",
        "            BIND (wd:{items} AS ?item) .\n",
        "            SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "            FILTER (LANG (?itemDescription) = \"en\")\n",
        "            }}\n",
        "    \"\"\"\n",
        "    # Choosing a random user agent from the list\n",
        "    user_agent = random.choice(user_agents)\n",
        "    # Sending the request to the Wikidata endpoint with the user agent header\n",
        "    response = requests.get(\"https://query.wikidata.org/sparql\", params={\"query\": query, \"format\": \"json\"}, headers={\"User-Agent\": user_agent})\n",
        "    # Parsing the response as JSON\n",
        "    data = response.json()\n",
        "\n",
        "    # Initializing an empty dictionary to store the information\n",
        "    info = {}\n",
        "    # Looping through the results\n",
        "    for result in data[\"results\"][\"bindings\"]:\n",
        "        # Adding the company's QID to the dictionary if exists\n",
        "        if \"QID\" in result:\n",
        "            info[\"QID\"] = result[\"QID\"][\"value\"]\n",
        "        # Adding the company label to the dictionary if exists\n",
        "        if \"itemLabel\" in result:\n",
        "            info[\"name\"] = result[\"itemLabel\"][\"value\"]\n",
        "        # Adding the industry label to the dictionary if exists\n",
        "        if \"itemDescription\" in result:\n",
        "            info[\"descr\"] = result[\"itemDescription\"][\"value\"]\n",
        "    # Returning the dictionary of information\n",
        "    return info\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_each_df(df):\n",
        "  import pandas as pd\n",
        "  # convert endtime column to string and filter out rows with valid dates\n",
        "  if 'endtime' in df.columns:\n",
        "    df['endtime'] = df['endtime'].astype(str)\n",
        "    df1 = df[~df['endtime'].str.contains('\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z')]\n",
        "  new_df =pd.DataFrame(columns=df.columns)\n",
        "  # create a set of unique QID and obQID pairs\n",
        "  qid1_qid2_pairs = set(zip(df1.iloc[:, 1], df1.iloc[:, 6]))\n",
        "  # loop over the unique pairs and select the last row for each pair\n",
        "  for qid1, qid2 in qid1_qid2_pairs:\n",
        "      temp_df = df1.loc[(df1.iloc[:, 1] == qid1) & (df1.iloc[:, 6] == qid2)]\n",
        "      temp_df = temp_df.drop_duplicates().reset_index(drop = True)\n",
        "      if len(temp_df) == 1:\n",
        "          new_row = temp_df.iloc[0]\n",
        "          new_df = pd.concat([new_df, new_row.to_frame().T])\n",
        "      if len(temp_df) >= 2:\n",
        "          # check if proportionofLabel column is in temp_df\n",
        "          if 'proportionofLabel' in temp_df:\n",
        "              # check if proportionofLabel column has any value\n",
        "              if any(temp_df['proportionofLabel']):\n",
        "                  # use list comprehension to filter rows with proportionofLabel equal to 'authorised capital'\n",
        "                  auth_cap_rows = [row for row in temp_df.itertuples() if row.proportionofLabel == 'authorised capital']\n",
        "                  if auth_cap_rows:\n",
        "                      # select the last row from the filtered list\n",
        "                      new_row = auth_cap_rows[-1]\n",
        "                      new_df = pd.concat([new_df, pd.Series(new_row._asdict()).to_frame().T])\n",
        "          else:\n",
        "              # return auth_cap_rows empty\n",
        "              auth_cap_rows = []\n",
        "          # use list comprehension to filter rows with pointoftime or starttime containing valid dates\n",
        "          date_rows = [row for row in temp_df.itertuples() if '\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z' in (row.pointoftime or row.starttime)]\n",
        "          if date_rows:\n",
        "              # use max function to find the latest date from the filtered list\n",
        "              latest_date = max(date_rows, key=lambda x: x.pointoftime or x.starttime)\n",
        "              new_row = latest_date\n",
        "              new_df = pd.concat([new_df, pd.Series(new_row._asdict()).to_frame().T])\n",
        "  # reset index and drop duplicates\n",
        "  new_df.reset_index(drop=True, inplace=True)\n",
        "  new_df = new_df.drop_duplicates().reset_index(drop = True)\n",
        "\n",
        "  # add those relations that has an endtime to the new dataframes\n",
        "  if 'endtime' in df.columns:\n",
        "    df['endtime'] = df['endtime'].astype(str)\n",
        "    df2 = df[df['endtime'].str.contains('\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z')]\n",
        "    df2.loc[:, 'proportion'] = 4\n",
        "    df2['endtime'] = df2['endtime'].str[:10]\n",
        "    df2 = df2.drop_duplicates()\n",
        "  new_df = pd.concat([new_df, df2]).reset_index(drop = True)\n",
        "\n",
        "  return new_df\n"
      ],
      "metadata": {
        "id": "AzS4v0cfOrP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_dupl_joined(new_df2):\n",
        "  ### remove the duplicates from the joined datasets\n",
        "  # Step 1. save the single parent-child pairs in the new3_df2\n",
        "  #         if there are 2 or more of the same parent-child pair, then \\\n",
        "  #         drop the ones within these that has blank p_proportion.\n",
        "  # Step 2. in a new list comprehension (temp_step2_df) save the ones that now became single parent-child.\n",
        "  #         if there are 2 or more and if one of the p_prop value is == 2, then \\\n",
        "  ##        drop this row with value == 2\n",
        "  #         Finally, if  there are 2 or more and their p_prop value is equal, then keep only the last\n",
        "  ##        (this will allow more values to pass through if they differ)\n",
        "  # STEP 1\n",
        "  # Use a set to store the unique cQID values\n",
        "  cQID_set = set(new_df2['cQID'])\n",
        "  # Use a generator expression to iterate over the pQID values\n",
        "  pQID_gen = (i for i in new_df2['pQID'])\n",
        "  # Use a list comprehension to filter the rows based on pQID and cQID values\n",
        "  temp_new3_df2 = [new_df2.loc[(new_df2['pQID'] == i) & (new_df2['cQID'] == j)]\n",
        "                  for i in pQID_gen for j in cQID_set]\n",
        "  # Use another list comprehension to process the filtered rows based on the\n",
        "  # length and p_proportion values\n",
        "  new3_df2 = pd.concat([row.iloc[0].to_frame().T if len(row) == 1\n",
        "                        else row.dropna(subset=['p_proportion']).loc[row['p_proportion'].notnull()]\n",
        "                        for row in temp_new3_df2])\n",
        "  new3_df2 = new3_df2.drop_duplicates().reset_index(drop = True)\n",
        "  # STEP 2\n",
        "  # Use a set to store the unique cQID values\n",
        "  cQID_set2 = set(new3_df2['cQID'])\n",
        "  # Use a generator expression to iterate over the pQID values\n",
        "  pQID_gen2 = (i for i in new3_df2['pQID'])\n",
        "  # Use a list comprehension to filter the rows based on pQID and cQID values\n",
        "  temp_step2_df = [new3_df2.loc[(new3_df2['pQID'] == i) & (new3_df2['cQID'] == j)]\n",
        "                  for i in pQID_gen2 for j in cQID_set2]\n",
        "  # Drop the rows with p_proportion value equal to 2\n",
        "  final_df = pd.concat([row.iloc[0].to_frame().T if len(row) == 1 \\\n",
        "                        else row.loc[row['p_proportion'] != 2] for row in temp_step2_df])\n",
        "  # Keep only the last row if there are multiple rows with the same pQID and cQID\n",
        "  # values and equal p_proportion values\n",
        "  final_df = final_df.drop_duplicates(subset=['pQID', 'cQID', 'p_proportion'], keep='last')\n",
        "  final_df = final_df.drop_duplicates().reset_index(drop = True)\n",
        "  # fill the blank p_proportion values with number 3, this will mean unknown on the graph\n",
        "  final_df['p_proportion'] = final_df['p_proportion'].fillna(3)\n",
        "  return final_df"
      ],
      "metadata": {
        "id": "YH2zLB4PXd-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_industries(final_df):\n",
        "  import numpy as np\n",
        "\n",
        "  # create a dictionary of industry frequencies\n",
        "  # split industry columns by \";\" and create separate columns\n",
        "  df_split1 = final_df[\"p_industries\"].str.split(\";\", expand=True)\n",
        "  df_split2 = final_df[\"c_industries\"].str.split(\";\", expand=True)\n",
        "  # join the two dataframes horizontally\n",
        "  df_split = pd.concat([df_split1, df_split2], axis=1)\n",
        "  # reshape the dataframe so that each expression is in a single column\n",
        "  df_melt = df_split.melt(value_name=\"expression\")\n",
        "  # drop the variable column\n",
        "  df_melt = df_melt.drop(\"variable\", axis=1)\n",
        "  # count the frequency of each expression and convert to dictionary\n",
        "  freq = df_melt[\"expression\"].value_counts().to_dict()\n",
        "\n",
        "  # define a custom function to pick the expression with highest frequency\n",
        "  def pick_expression(lst):\n",
        "    import numpy as np\n",
        "    # split the list by semicolon\n",
        "    lst = lst.split(\";\")\n",
        "    # initialize the best expression and frequency\n",
        "    best_exp = None\n",
        "    best_freq = 0\n",
        "    # loop through the list\n",
        "    for exp in lst:\n",
        "        # get the frequency of the expression\n",
        "        freq_exp = freq.get(exp, 0)\n",
        "        # if the frequency is higher than the best frequency, update the best expression and frequency\n",
        "        if freq_exp > best_freq:\n",
        "            best_exp = exp\n",
        "            best_freq = freq_exp\n",
        "    # return the best expression\n",
        "    return best_exp\n",
        "\n",
        "  # apply the custom function to each cell in col2\n",
        "  final_df[\"top_p_industries\"] = final_df[\"p_industries\"].apply(pick_expression)\n",
        "  final_df[\"top_c_industries\"] = final_df[\"c_industries\"].apply(pick_expression)\n",
        "\n",
        "  # replace the empty strings with NaN\n",
        "  final_df[\"top_p_industries\"] = final_df[\"top_p_industries\"].replace(\"\", np.nan)\n",
        "  final_df[\"top_c_industries\"] = final_df[\"top_c_industries\"].replace(\"\", np.nan)\n",
        "  final_df[\"p_industries\"] = final_df[\"p_industries\"].replace(\"\", np.nan)\n",
        "  final_df[\"c_industries\"] = final_df[\"c_industries\"].replace(\"\", np.nan)\n",
        "\n",
        "  # use isna method to get a boolean mask of missing values\n",
        "  mask = final_df[\"top_p_industries\"].isna()\n",
        "  mask2 = final_df[\"top_c_industries\"].isna()\n",
        "  mask3 = final_df[\"p_industries\"].isna()\n",
        "  mask4 = final_df[\"c_industries\"].isna()\n",
        "\n",
        "  # use loc method to assign \"unknown\" to the rows where col2 is missing\n",
        "  final_df.loc[mask, \"top_p_industries\"] = \"unknown\"\n",
        "  final_df.loc[mask2, \"top_c_industries\"] = \"unknown\"\n",
        "  final_df.loc[mask3, \"p_industries\"] = \"unknown\"\n",
        "  final_df.loc[mask4, \"c_industries\"] = \"unknown\"\n",
        "\n",
        "  final_df[\"top_p_industries\"] = final_df[\"top_p_industries\"].str.lstrip()\n",
        "  final_df[\"top_c_industries\"] = final_df[\"top_c_industries\"].str.lstrip()\n",
        "\n",
        "  # make a list of all those parent QIDs where the industry is \"unknown\"\n",
        "  unique_values1 = final_df.loc[(final_df['top_p_industries'] == 'unknown'), 'pQID'].drop_duplicates().values.tolist()\n",
        "  # making sure only those QIDs in unique_values that starts with \"Q\"\n",
        "  unique_values1 = [item for item in unique_values1 if item.startswith(\"Q\")]\n",
        "  # create a dataframe for the results\n",
        "  p_human_df1 = pd.DataFrame(columns=[\"QID\", \"name\", \"descr\"])\n",
        "  #initialising a new list to store the results\n",
        "  results =[]\n",
        "  # Loop through the values\n",
        "  for id in unique_values1:\n",
        "    # Run the module with the value as input and get the result\n",
        "    result = pd.DataFrame(is_human(id), index=[0])\n",
        "    # Appending the new dataframe to the list\n",
        "    results.append(result)\n",
        "\n",
        "  # join the new results to the dataframe\n",
        "  p_human_df1 = pd.concat([p_human_df1] + results, axis=0, join='outer').drop_duplicates().reset_index(drop = True)\n",
        "\n",
        "  #apply humans and their descriptions to new_df2 dataframe\n",
        "  for index, row in final_df.iterrows():\n",
        "      if row['pQID'] in p_human_df1['QID'].values:\n",
        "          # get the index of the matching row in p_human_df1\n",
        "          match_index = p_human_df1[p_human_df1['QID'] == row['pQID']].index\n",
        "          # get the value from the \"descr\" column of p_human_df1\n",
        "          descr_value = p_human_df1.loc[match_index[0], 'descr']\n",
        "          # update the values in new_df2\n",
        "          final_df.loc[index, 'p_industries'] = descr_value\n",
        "          final_df.loc[index, 'top_p_industries'] = \"human\"\n",
        "\n",
        "  # make a list of all those QIDs where the industry is \"unknown\"\n",
        "  unique_values3 = final_df.loc[(final_df['top_p_industries'] == 'unknown'), 'pQID'].drop_duplicates().values.tolist()\n",
        "  unique_values4 = final_df.loc[(final_df['top_c_industries'] == 'unknown'), 'cQID'].drop_duplicates().values.tolist()\n",
        "  # making sure only those QIDs in unique_values that starts with \"Q\"\n",
        "  unique_values3 = [item for item in unique_values3 if item.startswith(\"Q\")]\n",
        "  unique_values4 = [item for item in unique_values4 if item.startswith(\"Q\")]\n",
        "  # create a dataframe for the results\n",
        "  p_item_df1 = pd.DataFrame(columns=[\"QID\", \"name\", \"descr\"])\n",
        "  #initialising a new list to store the results\n",
        "  results =[]\n",
        "  # Loop through the values\n",
        "  for id in unique_values3:\n",
        "    # Run the module with the value as input and get the result\n",
        "    result = pd.DataFrame(get_description(id), index=[0])\n",
        "    # Appending the new dataframe to the list\n",
        "    results.append(result)\n",
        "\n",
        "  # join the new results to the dataframe\n",
        "  p_item_df1 = pd.concat([p_item_df1] + results, axis=0, join='outer').drop_duplicates().reset_index(drop = True)\n",
        "\n",
        "  # create a dataframe for the results\n",
        "  c_item_df2 = pd.DataFrame(columns=[\"QID\", \"name\", \"descr\"])\n",
        "  #initialising a new list to store the results\n",
        "  results2 =[]\n",
        "  # Loop through the values\n",
        "  for id2 in unique_values4:\n",
        "    # Run the module with the value as input and get the result\n",
        "    result2 = pd.DataFrame(get_description(id2), index=[0])\n",
        "    # Appending the new dataframe to the list\n",
        "    results2.append(result2)\n",
        "\n",
        "  # join the new results to the dataframe\n",
        "  c_item_df2 = pd.concat([c_item_df2] + results2, axis=0, join='outer').drop_duplicates().reset_index(drop = True)\n",
        "\n",
        "  #apply descriptions for unknown parents to new_df2 dataframe\n",
        "  for index, row in final_df.iterrows():\n",
        "      if row['pQID'] in p_item_df1['QID'].values:\n",
        "          # get the index of the matching row in p_human_df1\n",
        "          match_index = p_item_df1[p_item_df1['QID'] == row['pQID']].index\n",
        "          # get the value from the \"descr\" column of p_human_df1\n",
        "          descr_value = p_item_df1.loc[match_index[0], 'descr']\n",
        "          # update the values in new_df2\n",
        "          final_df.loc[index, 'p_industries'] = descr_value\n",
        "          final_df.loc[index, 'top_p_industries'] = \"other\"\n",
        "\n",
        "  #apply descriptions for unknown children to new_df2 dataframe\n",
        "  for index, row in final_df.iterrows():\n",
        "      if row['cQID'] in c_item_df2['QID'].values:\n",
        "          # get the index of the matching row in p_human_df1\n",
        "          match_index2 = c_item_df2[c_item_df2['QID'] == row['cQID']].index\n",
        "          # get the value from the \"descr\" column of p_human_df1\n",
        "          descr_value2 = c_item_df2.loc[match_index2[0], 'descr']\n",
        "          # update the values in new_df2\n",
        "          final_df.loc[index, 'c_industries'] = descr_value2\n",
        "          final_df.loc[index, 'top_c_industries'] = \"other\"\n",
        "\n",
        "  # convert the values in columns to strings\n",
        "  final_df['p_industries'] = final_df['p_industries'].astype(str)\n",
        "  final_df['top_p_industries'] = final_df['top_p_industries'].astype(str)\n",
        "  final_df['c_industries'] = final_df['c_industries'].astype(str)\n",
        "  final_df['top_c_industries'] = final_df['top_c_industries'].astype(str)\n",
        "  return final_df"
      ],
      "metadata": {
        "id": "FwrR8XelzIQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a function to convert the known country names to the continents they belong\n",
        "# should first install pycountry and pycountry_convert\n",
        "def country_to_continent(country_name):\n",
        "  import pycountry_convert as pc\n",
        "  if country_name == 'unknown':\n",
        "    return 'unknown'\n",
        "  else:\n",
        "    country_alpha2 = pc.country_name_to_country_alpha2(country_name)\n",
        "    country_continent_code = pc.country_alpha2_to_continent_code(country_alpha2)\n",
        "    country_continent_name = pc.convert_continent_code_to_continent_name(country_continent_code)\n",
        "    return country_continent_name\n"
      ],
      "metadata": {
        "id": "z-8KWbNPM6S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prepare dataset main function: clean_join(p_df, ob_df, s_df, oo_df)"
      ],
      "metadata": {
        "id": "bAq56Sgx9BND"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEIDYYq5XeZp"
      },
      "outputs": [],
      "source": [
        "# clean each dataset and then join them\n",
        "def clean_join(p_df, ob_df, s_df, oo_df):\n",
        "  #global final_df, new_p_df, new_ob_df, new_s_df, new_oo_df\n",
        "  # convert the data columns to strings\n",
        "  p_df.loc[:, ['pointoftime', 'starttime']] = p_df.loc[:, ['pointoftime', 'starttime']].astype(str)\n",
        "  ob_df.loc[:, ['pointoftime', 'starttime']] = ob_df.loc[:, ['pointoftime', 'starttime']].astype(str)\n",
        "  s_df.loc[:, ['pointoftime', 'starttime']] = s_df.loc[:, ['pointoftime', 'starttime']].astype(str)\n",
        "  oo_df.loc[:, ['pointoftime', 'starttime']] = oo_df.loc[:, ['pointoftime', 'starttime']].astype(str)\n",
        "  ### remove the duplicates within p dataset - keep the last\n",
        "  new_p_df = pd.DataFrame(columns=['item', 'QID', 'itemLabel', 'itemcountryLabel', 'industries', 'parent', \\\n",
        "                                    'pQID', 'parentLabel', 'parentcountryLabel', 'pindustries', 'proportion', \\\n",
        "                                    'proportionofLabel', 'pointoftime', 'starttime', 'endtime'])\n",
        "  new_p_df = pd.concat([new_p_df, clear_each_df(p_df)]).reset_index(drop=True)\n",
        "\n",
        "  ### remove the duplicates within ob dataset - keep the last\n",
        "  new_ob_df = pd.DataFrame(columns=['item', 'QID', 'itemLabel', 'itemcountryLabel', 'industries', 'ownedby', \\\n",
        "                                    'obQID', 'ownedbyLabel', 'ownedbycountryLabel', 'obindustries', 'proportion', \\\n",
        "                                    'proportionofLabel', 'pointoftime', 'starttime', 'endtime'])\n",
        "  new_ob_df = pd.concat([new_ob_df, clear_each_df(ob_df)]).reset_index(drop=True)\n",
        "\n",
        "  ### remove the duplicates within s dataset - keep the last\n",
        "  new_s_df = pd.DataFrame(columns=['item', 'QID', 'itemLabel', 'itemcountryLabel', 'industries', 'subsidiary', \\\n",
        "                                    'sQID', 'subsidiaryLabel', 'subsidiarycountryLabel', 'sindustries', 'proportion', \\\n",
        "                                    'proportionofLabel', 'pointoftime', 'starttime', 'endtime'])\n",
        "  new_s_df = pd.concat([new_s_df, clear_each_df(s_df)]).reset_index(drop=True)\n",
        "\n",
        "  ### remove the duplicates within oo dataset - keep the last\n",
        "  new_oo_df = pd.DataFrame(columns=['item', 'QID', 'itemLabel', 'itemcountryLabel', 'industries', 'ownerof', \\\n",
        "                                    'ooQID', 'ownerofLabel', 'ownerofcountryLabel', 'ooindustries', 'proportion', \\\n",
        "                                    'proportionofLabel', 'pointoftime', 'starttime', 'endtime'])\n",
        "  new_oo_df = pd.concat([new_oo_df, clear_each_df(oo_df)]).reset_index(drop=True)\n",
        "\n",
        "  ### join together the dataframes into a new_df\n",
        "  new_df = pd.DataFrame(columns=[\"pQID\", \"parent\", \"parent_country\", \"p_industries\", \"cQID\", \"child\", \\\n",
        "                                \"child_country\", \"c_industries\", \"p_proportion\", \"proportionofLabel\", \\\n",
        "                                \"pointoftime\", \"starttime\", \"endtime\"])\n",
        "  # Create an empty list to store the new rows\n",
        "  new_p_rows = []\n",
        "  # Loop over the new_p_df dataframe\n",
        "  for i in range(len(new_p_df)):\n",
        "      # Create a new row as a dictionary\n",
        "      new_p_row = {'pQID': new_p_df['pQID'][i], 'parent': new_p_df['parentLabel'][i], 'parent_country': new_p_df['parentcountryLabel'][i], \\\n",
        "                'p_industries': new_p_df['pindustries'][i], 'cQID': new_p_df['QID'][i], \\\n",
        "                'child': new_p_df['itemLabel'][i], 'child_country': new_p_df['itemcountryLabel'][i], 'c_industries': new_p_df['industries'][i], \\\n",
        "                'p_proportion':new_p_df['proportion'][i], 'proportionofLabel':new_p_df['proportionofLabel'][i], \\\n",
        "                'pointoftime':new_p_df['pointoftime'][i], 'starttime':new_p_df['starttime'][i], 'endtime': new_p_df['endtime'][i]}\n",
        "      # Convert the dictionary to a dataframe and append it to the list\n",
        "      new_p_rows.append(pd.DataFrame(new_p_row, index=[0]))\n",
        "  # Concatenate the list of dataframes with the original dataframe\n",
        "  new_df = pd.concat([new_df] + new_p_rows, ignore_index=True)\n",
        "  new_df['p_proportion'] = new_df['p_proportion'].fillna(2)\n",
        "  # Create an empty list to store the new rows\n",
        "  new_ob_rows = []\n",
        "  for i in range(len(new_ob_df)):\n",
        "      new_ob_row = {'pQID': new_ob_df['obQID'][i], 'parent': new_ob_df['ownedbyLabel'][i], 'parent_country': new_ob_df['ownedbycountryLabel'][i], \\\n",
        "                'p_industries': new_ob_df['obindustries'][i], 'cQID': new_ob_df['QID'][i], \\\n",
        "                'child': new_ob_df['itemLabel'][i], 'child_country': new_ob_df['itemcountryLabel'][i], 'c_industries': new_ob_df['industries'][i], \\\n",
        "                'p_proportion':new_ob_df['proportion'][i], 'proportionofLabel':new_ob_df['proportionofLabel'][i], \\\n",
        "                'pointoftime':new_ob_df['pointoftime'][i], 'starttime':new_ob_df['starttime'][i], 'endtime': new_ob_df['endtime'][i]}\n",
        "      new_ob_rows.append(pd.DataFrame(new_ob_row, index=[0]))\n",
        "  # Concatenate the list of dataframes with the original dataframe\n",
        "  new_df = pd.concat([new_df] + new_ob_rows, ignore_index=True)\n",
        "  # Create an empty list to store the new rows\n",
        "  new_s_rows = []\n",
        "  for i in range(len(new_s_df)):\n",
        "      new_s_row = {'pQID': new_s_df['QID'][i], 'parent': new_s_df['itemLabel'][i], 'parent_country': new_s_df['itemcountryLabel'][i], \\\n",
        "                'p_industries': new_s_df['industries'][i], 'cQID': new_s_df['sQID'][i], \\\n",
        "                'child': new_s_df['subsidiaryLabel'][i], 'child_country': new_s_df['subsidiarycountryLabel'][i], 'c_industries': new_s_df['sindustries'][i], \\\n",
        "                'p_proportion':new_s_df['proportion'][i], 'proportionofLabel':new_s_df['proportionofLabel'][i], \\\n",
        "                'pointoftime':new_s_df['pointoftime'][i], 'starttime':new_s_df['starttime'][i], 'endtime': new_s_df['endtime'][i]}\n",
        "      new_s_rows.append(pd.DataFrame(new_s_row, index=[0]))\n",
        "  # Concatenate the list of dataframes with the original dataframe\n",
        "  new_df = pd.concat([new_df] + new_s_rows, ignore_index=True)\n",
        "  # Create an empty list to store the new rows\n",
        "  new_oo_rows = []\n",
        "  for i in range(len(new_oo_df)):\n",
        "      new_oo_row = {'pQID': new_oo_df['QID'][i], 'parent': new_oo_df['itemLabel'][i], 'parent_country': new_oo_df['itemcountryLabel'][i], \\\n",
        "                'p_industries': new_oo_df['industries'][i], 'cQID': new_oo_df['ooQID'][i], \\\n",
        "                'child': new_oo_df['ownerofLabel'][i], 'child_country': new_oo_df['ownerofcountryLabel'][i], 'c_industries': new_oo_df['ooindustries'][i], \\\n",
        "                'p_proportion':new_oo_df['proportion'][i], 'proportionofLabel':new_oo_df['proportionofLabel'][i], \\\n",
        "                'pointoftime':new_oo_df['pointoftime'][i], 'starttime':new_oo_df['starttime'][i], 'endtime': new_oo_df['endtime'][i]}\n",
        "      new_oo_rows.append(pd.DataFrame(new_oo_row, index=[0]))\n",
        "  # Concatenate the list of dataframes with the original dataframe\n",
        "  new_df = pd.concat([new_df] + new_oo_rows, ignore_index=True)\n",
        "  new_df2 = new_df.drop_duplicates().reset_index(drop = True)\n",
        "\n",
        "  # drop the duplicates from the joined dataframe with various rules defined by drop_dupl_joined() function\n",
        "  final_df = drop_dupl_joined(new_df2)\n",
        "  # clear up industries and find the top ones for classification. unknown ones: get description (also checks if its human)\n",
        "  final_df = top_industries(final_df)\n",
        "\n",
        "  # replace missing country values with \"unknown\"\n",
        "  final_df[\"parent_country\"] = final_df[\"parent_country\"].replace(\"\", np.nan)\n",
        "  final_df[\"child_country\"] = final_df[\"child_country\"].replace(\"\", np.nan)\n",
        "  mask_p = final_df[\"parent_country\"].isna()\n",
        "  mask_c = final_df[\"child_country\"].isna()\n",
        "  final_df.loc[mask_p, \"parent_country\"] = \"unknown\"\n",
        "  final_df.loc[mask_c, \"child_country\"] = \"unknown\"\n",
        "  # apply the country_to_continent function to get the continents the companies are located\n",
        "  final_df['parent_continent'] = final_df['parent_country'].apply(country_to_continent)\n",
        "  final_df['child_continent'] = final_df['child_country'].apply(country_to_continent)\n",
        "  return final_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn2ym2t8hnxV"
      },
      "source": [
        "### function 3: visualise_b2b_network(final_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bf7r3dPYsuFU"
      },
      "outputs": [],
      "source": [
        "def visualise_b2b_network(final_df):\n",
        "  # import packages\n",
        "  import pandas as pd\n",
        "  from pyvis.network import Network\n",
        "  import seaborn as sns\n",
        "  import colorsys\n",
        "  from collections import Counter\n",
        "  import json\n",
        "\n",
        "  # create network object\n",
        "  net = Network(\n",
        "      notebook = True,\n",
        "      directed = True,            # directed graph\n",
        "      bgcolor = \"snow\",          # background color of graph\n",
        "      font_color = \"navy\",        # use navy for node labels\n",
        "      cdn_resources = 'in_line',  # make sure Jupyter notebook can display correctly\n",
        "      height = \"1000px\",          # height of chart\n",
        "      width = \"100%\",             # fill the entire width\n",
        "      select_menu=True,           # user can choose from campany name list\n",
        "      filter_menu=True,           # user can search for colors - indicators for industries\n",
        "      neighborhood_highlight=True, # clicking on a node highlights its connections and grays out others\n",
        "      )\n",
        "\n",
        "  # create lists of nodes and edges from dataframe columns\n",
        "  nodes = []\n",
        "  edges = []\n",
        "  for parent, child, p_industry, c_industry, top_p_industry, top_c_industry, parent_country, parent_continent, \\\n",
        "      child_country, child_continent, proportion, endtime in zip(final_df['parent'], \\\n",
        "                                                                 final_df['child'], \\\n",
        "                                                                 final_df['p_industries'], \\\n",
        "                                                                 final_df['c_industries'], \\\n",
        "                                                                 final_df['top_p_industries'], \\\n",
        "                                                                 final_df['top_c_industries'], \\\n",
        "                                                                 final_df['parent_country'], \\\n",
        "                                                                 final_df['parent_continent'], \\\n",
        "                                                                 final_df['child_country'], \\\n",
        "                                                                 final_df['child_continent'], \\\n",
        "                                                                 final_df['p_proportion'], \\\n",
        "                                                                 final_df['endtime']):\n",
        "      nodes.append((parent, p_industry, top_p_industry, parent_country, parent_continent))\n",
        "      nodes.append((child, c_industry, top_c_industry, child_country, child_continent))\n",
        "      edges.append((parent, child, proportion, endtime))\n",
        "\n",
        "  # remove duplicate nodes\n",
        "  #nodes = list(set(nodes))\n",
        "\n",
        "  # create a set of unique industries\n",
        "  industries = set(final_df['top_p_industries']).union(set(final_df['top_c_industries']))\n",
        "\n",
        "  # get the number of unique industries\n",
        "  n_colors = len(industries)\n",
        "\n",
        "  # create a dictionary that maps each unique industry to a pastel color\n",
        "  pastel_colors = sns.color_palette('muted', n_colors)\n",
        "\n",
        "  # define a function that converts an rgb tuple to an rgba string with a pastel factor\n",
        "  def rgb_to_rgba(rgb, pastel_factor):\n",
        "      # convert rgb values to hls values\n",
        "      h, l, s = colorsys.rgb_to_hls(*rgb)\n",
        "      # increase lightness by multiplying with pastel factor\n",
        "      l *= pastel_factor\n",
        "      # convert hls values back to rgb values\n",
        "      r, g, b = colorsys.hls_to_rgb(h, l, s)\n",
        "      # normalize rgb values to be between 0 and 255\n",
        "      r = int(r * 255)\n",
        "      g = int(g * 255)\n",
        "      b = int(b * 255)\n",
        "      # add alpha value of 0.7\n",
        "      a = 0.7\n",
        "      # format rgba values as a string\n",
        "      rgba = f'rgba({r}, {g}, {b}, {a})'\n",
        "      return rgba\n",
        "\n",
        "  group_color_map = {}\n",
        "  for i, top_industry in enumerate(industries):\n",
        "      # use the function with pastel_colors[i] and a pastel factor of 1.2\n",
        "      group_color_map[top_industry] = rgb_to_rgba(pastel_colors[i], 1.2)\n",
        "\n",
        "  # create a dictionary that maps each node to its degree\n",
        "  node_degree_map = Counter()\n",
        "  for edge in edges:\n",
        "      node_degree_map[edge[0]] += 1\n",
        "      node_degree_map[edge[1]] += 1\n",
        "\n",
        "  # define a function that takes a degree and a scaling factor and returns a size for the node\n",
        "  def degree_to_size(degree, scaling_factor):\n",
        "      # multiply degree by scaling factor to get base size\n",
        "      base_size = degree * scaling_factor\n",
        "      # add minimum size to base size to ensure no node is too small\n",
        "      min_size = 3\n",
        "      final_size = base_size + min_size\n",
        "      # return final size as an integer\n",
        "      return int(final_size)\n",
        "\n",
        "  # create a dictionary that maps each node to its size\n",
        "  node_size_map = {}\n",
        "  for node in node_degree_map:\n",
        "      # use the function with node_degree_map[node] and a scaling factor of 5\n",
        "      node_size_map[node] = degree_to_size(node_degree_map[node], 5)\n",
        "\n",
        "  # define a different color for the highlighted nodes\n",
        "  highlight_color = 'green'\n",
        "  #node1 = root_companies[0]\n",
        "  #node2 = root_companies[1]\n",
        "\n",
        "  # create a set of unique countries and continents\n",
        "  countries = set(final_df['parent_country']).union(set(final_df['child_country']))\n",
        "  continents = set(final_df['parent_continent']).union(set(final_df['child_continent']))\n",
        "\n",
        "  # add nodes and edges to network object\n",
        "  for node, industry, top_industry, countries, continents in nodes:\n",
        "      color = group_color_map[top_industry]\n",
        "      # check if the node name is equal to node1 or node2 and use highlight_color if so\n",
        "      #if node == root_companies:\n",
        "      #    color = highlight_color\n",
        "      #else:\n",
        "      #    color = group_color_map[top_industry]\n",
        "      # assign shapes for teh continents\n",
        "      if continents == 'North America':\n",
        "        node_shape = 'triangle'\n",
        "      elif continents == 'South America':\n",
        "        node_shape = 'triangleDown'\n",
        "      elif continents == 'Europe':\n",
        "        node_shape = 'star'\n",
        "      elif continents == 'Africa':\n",
        "        node_shape = 'diamond'\n",
        "      elif continents == 'Asia':\n",
        "        node_shape = 'square'\n",
        "      elif continents == 'Australia':\n",
        "        node_shape = 'ellipse'\n",
        "      else:\n",
        "        node_shape = 'dot'\n",
        "      size = node_size_map[node]\n",
        "      net.add_node(node, label=node, group=top_industry, color=color, shape=node_shape, title=node + ', '+ countries + ' (' + industry + ')', value=size)\n",
        "\n",
        "  for e in edges:\n",
        "      if e[2] > 0 and e[2] < 0.5:\n",
        "          net.add_edge(e[0], e[1], title=str(e[2]), arrows={\"to\": True}, color='turquoise') #value=(e[2]/100), dashes=[6,10,1,10]\n",
        "      elif e[2] >= 0.5 and e[2] <= 1:\n",
        "          net.add_edge(e[0], e[1], title=str(e[2]), arrows={\"to\": True}, color='violet') #value=(e[2]/10),  dashes=False\n",
        "      elif e[2] == 2:\n",
        "          net.add_edge(e[0], e[1], title=\"parent\", arrows={\"to\": True}, color='salmon') #dashes=[5,5]\n",
        "      elif e[2] == 3:\n",
        "          net.add_edge(e[0], e[1], title=\"unknown value\", arrows={\"to\": True}, color='lime') # dashes=[1,10]\n",
        "      elif e[2] == 4:\n",
        "          net.add_edge(e[0], e[1], title=\"end date: \"+str(e[3]), arrows={\"to\": True}, color='grey')\n",
        "\n",
        "\n",
        "  net.repulsion(\n",
        "      node_distance=100,\n",
        "      central_gravity=0.2,\n",
        "      spring_length=200,\n",
        "      spring_strength=0.05,\n",
        "      damping=0.09,\n",
        "  )\n",
        "\n",
        "  net.show_buttons(filter_='physics')\n",
        "  #net.show_buttons(filter_=['nodes', 'edges', 'physics'])\n",
        "\n",
        "  # show network graph\n",
        "  return net.show('B2B_network_Wiki.html')"
      ]
    }
  ]
}
